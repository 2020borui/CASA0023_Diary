[
  {
    "objectID": "Wk1_Introduction_to_remote_sensing.html",
    "href": "Wk1_Introduction_to_remote_sensing.html",
    "title": "1  Introduction to remote sensing",
    "section": "",
    "text": "1.1 Summary\nThis week’s learning content can be divided into two parts: The first part is the concept of remote sensing and the process of gaining the data. Especially, discussing how the interaction between Electromagnetic radiation and Earth’s surface (reflecting, absorbing and scattering) influences the data quality. There are different impacts on electromagnetic radiation with different wavelengths, so it’s important to choose suitable bands to analyse. Moreover, the weather especially the clouds will influence on capturing data. SAR can go through the clouds to reduce the impacts.\nThe second parts introduce the type of remote data, and how to simply deal with data by plotting spectral signatures. In this practical we use snap and r to deal with Landsat and Sentinel data. The main difference between remote data is from four resolutions: spatial, spectral, temporal and radiometric.\nThe steps are:\nTasseled Caps",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to remote sensing</span>"
    ]
  },
  {
    "objectID": "Wk1_Introduction_to_remote_sensing.html#summary",
    "href": "Wk1_Introduction_to_remote_sensing.html#summary",
    "title": "1  Introduction to remote sensing",
    "section": "",
    "text": "remote sensing process Source: Remote Sensing for Dummies\n\n\n\n\n\n\nDownloading data with less cloud cover, choosing different band combinations for different visuals\n\n\nMasking the study area and resampling the data in one spatial resolution using the nearest neighbour. Choosing down-scaling to avoid errors when calculating. The nearest neighbour is more effective and faster than bilinear or cubic convolution. But there will be abrupt changes at the boundary leading to large errors. When deciding to resample the resolution should not change too much which will influence the accuracy of the result.\n\n\nSelect POIs, and create vector data containers for different land cover.\n\n\nDraw vector, export the Geo-tiff and shapefile load the file in R studio, and extract the value of tiff by vector.\n\n\nPlotting the Spectral signatures and comparing the difference of two sensors with the same band and same land use by t-test. The value of Landsat is much greater than that of Sentinel.\n\n\n\n\n\n\n\n\n\n\n\nLandsat Spectral signatures\n\n\n\n\n\n\n\nSentinel Spectral signatures",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to remote sensing</span>"
    ]
  },
  {
    "objectID": "Wk1_Introduction_to_remote_sensing.html#applications",
    "href": "Wk1_Introduction_to_remote_sensing.html#applications",
    "title": "1  Introduction to remote sensing",
    "section": "1.2 Applications",
    "text": "1.2 Applications\nI’m interested in remote sensing in vegetation monitoring. The main difference between the two sensors Landsat and Sentinel is in the various resolutions on spatial, spectral and temporal. By comparing two articles about machine learning used in vegetation monitoring, I observed that two types of data perform differently in different tasks. Sentinel data performs more accurately in predicting by regression on a small scale, the Landsat data is more accurate in classifying vegetation types over large areas.\nIt may be due to the following reasons: Sentinel has a red edge band (B5, B6 and B7) that is more sensitive to monitoring vegetation. It also has a higher resolution of around 10M and 20M, so it has a more accurate prediction of vegetation characteristics such as trunk height for forest variables. (Astola et al. 2019) Sentinel has a 5-day return cycle, so, it captures short-term changes in vegetation well, but it will be complicated to handle large ranges of data. As a new sensor from 2015, it may be a lack of data build-up.\n\n\n\nModel accuracy for different data sources Source: Ram C. Sharma’s article\n\n\nTo my surprise, Landsat data are more accurate in studies related to the classification of vegetation types by setting a random forest model. (Sharma, Hara, and Tateishi 2017)Although Landsat has a 16-day return cycle, due to the length of time covered it has rich datasets and it has SWIR and thermal infrared bands, both of which can provide help for the study of large-scale classification. At the end of the article, the author got a more precise classification by combination of the two types of data.\nHow can these methods be applied to real life? Scientists found that near-infrared radiation can observe the health of vegetation. It is based on the principle that the health status of plants will influence the plant’s spectrum of both absorption and reflection. The U.S. Department of Agriculture uses Landsat and the U.S. Geological Survey to forecast agricultural productivity in each growing season. (Blickensdörfer et al. 2022)According to the comparison above, I think the result will be better to use Landsat data for large-scale farmland species identification, and then use Sentinel data to monitor small-scale changes in the growing season of a single crop.\n\n\n\nThe difference between Sentinel-2 and Landsat 8\n\n\nComparison Factors\nSentinel-2\nLandsat 8\n\n\n\n\nTemporal Coverage\n~73 times/year (high frequency)\n~23 times/year (low frequency)\n\n\nSpatial Resolution\n10 m (selected bands), 20 m (red edge)\n30 m (except 15 m panchromatic)\n\n\nBand Configuration\nVisible, NIR, SWIR, red edge (no thermal infrared)\nVisible, NIR, SWIR, thermal infrared\n\n\nClassification Performance\nMulti-temporal features enhance vegetation health classification\nLong-term data improves stability, versatile band coverage\n\n\nAdvantages\nSuperior for small-scale, short-term changes (e.g., vegetation health)\nSuperior for large-scale, long-term changes (e.g., vegetation types)\n\n\nApplications\nBetter for specific feature prediction\nBetter for large-scale classification",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to remote sensing</span>"
    ]
  },
  {
    "objectID": "Wk1_Introduction_to_remote_sensing.html#reflection",
    "href": "Wk1_Introduction_to_remote_sensing.html#reflection",
    "title": "1  Introduction to remote sensing",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection\nThrough this week I have learnt about the principles of remote sensing and a deep understanding of the advantages and application of Sentinel-2 and Landsat 8 data. Considering that radiation is an important element in remote sensing, I am wondering how climate change affects remote sensing vegetation monitoring. With global warming and the greenhouse effect, will the increased thermal radiation in the atmosphere have impacts on remote sensing results? Will the increased plant transpiration due to warming affect the absorption of red light and the reflection of near-infrared light? Can remote sensing capture the slight changes? Does the use of thermal infrared bands in vegetation health monitoring need to be adapted due to global warming? These issues need to be addressed step by step in the future.\nAlthough there are massive studies about using and comparing Landsat and Sentinel-2 data on vegetation research, I am interested in how to innovate under the background of climate change. Could we build a model to predict the influence of climate on vegetation growth? Based on that we can predict how vegetation in a region or globally will be distributed over the next 50 years based on current climate change. It’s a big help to our food security and species conservation. Moreover, through this week’s studying, I also learned how to choose data with suitable resolution and choose a band which is more suitable for specific tasks. I think the ability to choose suitable data sets and methods is important in future study and learning.\n\n\n\n\nAstola, Heikki, Tuomas Häme, Laura Sirro, Matthieu Molinier, and Jorma Kilpi. 2019. “Comparison of Sentinel-2 and Landsat 8 Imagery for Forest Variable Prediction in Boreal Region.” Remote Sensing of Environment 223 (March): 257–73. https://doi.org/10.1016/j.rse.2019.01.019.\n\n\nBlickensdörfer, Lukas, Marcel Schwieder, Dirk Pflugmacher, Claas Nendel, Stefan Erasmi, and Patrick Hostert. 2022. “Mapping of Crop Types and Crop Sequences with Combined Time Series of Sentinel-1, Sentinel-2 and Landsat 8 Data for Germany.” Remote Sensing of Environment 269 (February): 112831. https://doi.org/10.1016/j.rse.2021.112831.\n\n\nSharma, Ram, Keitarou Hara, and Ryutaro Tateishi. 2017. “High-Resolution Vegetation Mapping in Japan by Combining Sentinel-2 and Landsat 8 Based Multi-Temporal Datasets Through Machine Learning and Cross-Validation Approach.” Land 6 (3): 50. https://doi.org/10.3390/land6030050.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to remote sensing</span>"
    ]
  },
  {
    "objectID": "Wk3_Corrections.html",
    "href": "Wk3_Corrections.html",
    "title": "3  Corrections and Enhancement",
    "section": "",
    "text": "3.1 Summary\nThe lecture includes two parts: Correction data and Accessing data. This is preparation for conducting data research.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Corrections and Enhancement</span>"
    ]
  },
  {
    "objectID": "Wk4_Policy.html",
    "href": "Wk4_Policy.html",
    "title": "4  Policy",
    "section": "",
    "text": "4.1 Summary\nNow, we are against the challenges of a changing climate. Disaster risk reduction (DRR) and sustainable development are important for all cities worldwide. Climate change has led to an increased risk of natural disasters, such as large-scale precipitation, coastal flooding, and sea-level rise, which pose significant challenges to the safety of urban residents and urban infrastructure. According to the projections from NPCC, at the end of the century, the frequency of 100-year floods will be up to 3.6%.\nNew York as a famous coastal city has hundreds of miles of waterfront and commercial harbours. It will face more emergency crises from flooding. By the 2050s, the sea levels rise in New York are twice the global average, nearly 1 million residents will be influenced by the coastal floodings, some commercial regions will be flooded without protective measures. (Fuleihan et al., n.d.) New York has already take many measure to face this emergency, for example, construction of marine parks, widening and elevation of the coastline, enhancement of flood prevention infrastructure, etc.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Policy</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "1  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Astola, Heikki, Tuomas Häme, Laura Sirro, Matthieu Molinier, and Jorma\nKilpi. 2019. “Comparison of Sentinel-2 and Landsat 8 Imagery for\nForest Variable Prediction in Boreal Region.” Remote Sensing\nof Environment 223 (March): 257–73. https://doi.org/10.1016/j.rse.2019.01.019.\n\n\nBlickensdörfer, Lukas, Marcel Schwieder, Dirk Pflugmacher, Claas Nendel,\nStefan Erasmi, and Patrick Hostert. 2022. “Mapping of Crop Types\nand Crop Sequences with Combined Time Series of Sentinel-1, Sentinel-2\nand Landsat 8 Data for Germany.” Remote Sensing of\nEnvironment 269 (February): 112831. https://doi.org/10.1016/j.rse.2021.112831.\n\n\nCohen, Sagy, Austin Raney, Dinuke Munasinghe, J. Derek Loftis, Andrew\nMolthan, Jordan Bell, Laura Rogers, et al. 2019. “The Floodwater\nDepth Estimation Tool (FwDET V2.0) for Improved Remote Sensing Analysis\nof Coastal Flooding.” Natural Hazards and Earth System\nSciences 19 (9): 2053–65. https://doi.org/10.5194/nhess-19-2053-2019.\n\n\nFuleihan, Dean, Dominic Williams, Daniel A Zarrilli, and OneNYC\nDirector. n.d. “The City of New York Mayor Bill de Blasio.”\n\n\nHall-Beyer, Mryka. 2017a. “Practical Guidelines for Choosing GLCM\nTextures to Use in Landscape Classification Tasks over a Range of\nModerate Spatial Scales.” International Journal of Remote\nSensing 38 (5): 1312–38. https://doi.org/10.1080/01431161.2016.1278314.\n\n\n———. 2017b. “Practical Guidelines for Choosing GLCM Textures to\nUse in Landscape Classification Tasks over a Range of Moderate Spatial\nScales.” International Journal of Remote Sensing 38 (5):\n1312–38. https://doi.org/10.1080/01431161.2016.1278314.\n\n\nMason, David C., Sarah L. Dance, and Hannah L. Cloke. 2023.\n“Toward Improved Urban Flood Detection Using Sentinel-1:\nDependence of the Ratio of Post- to Preflood Double Scattering Cross\nSections on Building Orientation.” Journal of Applied Remote\nSensing 17 (1): 16507. https://doi.org/10.1117/1.JRS.17.016507.\n\n\n“NYS Flood Impact Decision Support System - FIDSS.” n.d. https://sedac.ciesin.columbia.edu/mapping/nysfidss/?page=NYS-FIDSS&views=About.\n\n\nSharma, Ram, Keitarou Hara, and Ryutaro Tateishi. 2017.\n“High-Resolution Vegetation Mapping in Japan by Combining\nSentinel-2 and Landsat 8 Based Multi-Temporal Datasets Through Machine\nLearning and Cross-Validation Approach.” Land 6 (3): 50.\nhttps://doi.org/10.3390/land6030050.\n\n\nTabib Mahmoudi, Fatemeh, Alireza Arabsaeedi, and Seyed Kazem Alavipanah.\n2019. “Feature-Level Fusion of Landsat 8 Data and SAR Texture\nImages for Urban Land Cover Classification.” Journal of the\nIndian Society of Remote Sensing 47 (3): 479–85. https://doi.org/10.1007/s12524-018-0914-8.\n\n\n“THE 17 GOALS | Sustainable Development.” n.d. https://sdgs.un.org/goals.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CASA0023 Learning Diary",
    "section": "",
    "text": "Personal Introduction\nMy name is Borui. I am from Beijing. I did my undergraduate degree at China Agricultural University and majored in Landscape Architecture. During that time, I became interested in landscape design, which covered topics such as food security, climate change, urban segregation, etc. This is my portfolio from undergrad. I am also interested in the challenges that cities will face due to climate change, such as flooding, and chose a related topic for my undergraduate final project.\nNow, I am studying MSc in spatial data science. I hope to be able to analyse the problems in urban space using data sciences and to improve my quantitative analysis and coding skills.\nI took this module Remotely Sensing Cities and Environments to learn more about environmental hazards arising from a changing climate. Through learning and operationalising remotely sensed Earth observation data, I hope I can provide effective recommendations for today’s cities or urban planning.\nThis diary will record my learning process throughout the module. It includes a summary of knowledge from each lesson, practical application and personal reflective thinking. I hope to be able to show what I have learnt from this module in this way.",
    "crumbs": [
      "Personal Introduction"
    ]
  },
  {
    "objectID": "Wk2_Presentation.html",
    "href": "Wk2_Presentation.html",
    "title": "2  Xaringan Presentation",
    "section": "",
    "text": "This week we tried to use markdown tools like Xaringan and Quarto to show our idea on the website with code. How to adjust the templates and hide the code blocks was difficult for me during the learning process. Creating presentations through code gives the presentation file greater presentational reproducibility and flexibility. Although the process of learning is difficult, it can give me a strong sense of achievement. Here is a presentation that introduces the functions of the Sentinel-2, created through the Xaringan.\n\n\n\n\n\n\n\n\nView Full PowerPoint",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Xaringan Presentation</span>"
    ]
  },
  {
    "objectID": "Wk3_Corrections.html#summary",
    "href": "Wk3_Corrections.html#summary",
    "title": "3  Corrections and Enhancement",
    "section": "",
    "text": "3.1.1 Correction\nThe correction makes the raw remote-sensing data become “Analysis Ready Data”(ARD). In this lecture, we mainly learn four kinds of correction: Geometric Correction, Atmospheric Correction, Orthorectification, and Radiometric Correction. I try to figure out the connection between them to have a better understanding. It seems like the Radiometric correction and atmospheric correction are the most common correction steps in remote sensing data processing, and are usually required for every image. Geometric Correction and Orthorectification are commonly used processing scenarios where spatial aberrations exist or where accurate spatial analysis is required, not every data is needed. The following table will introduce details about different corrections. Regression is important in correction methods.\nSurface reflectance is the result of radiative and atmospheric corrections. In practice, we get the reflectance from DN (level 1) through DOS methods. Nowadays there are many ways to get processed data like Landsat8 (level 2-surface reflectance). This data has been atmospherically corrected. Saves a lot of work. But in the future, there will be some chance to handle high-resolution images that are not processed. It is still important to know how to correct it and how to tackle problems like aberrations. We should download the data that fits our needs. Using Landsat data as an example, the hierarchical structure of Landsat data is Collection→ Level→ Tier. In general, we choose Collection 2 + Level-2 + Tier 1.\n\n\n\n\n\nCorrection\npurposes /defination\nmethods\ninput data\noutput\nApplications\n\n\n\n\nGeometric Correction\nCorrects distortions caused by the sensor or satellites to align the image with the geographic coordinate system, ensuring spatial accuracy.\ndifferent transformation types(linear, polynomial, Helmert…)\nPixel coordinates of the original image, GCPs（Ground Control Points, Pixel coordinates of the rectified (gold standard) map\ncorrected image\nThe basis for subsequent georectification and orthorectification\n\n\nAtmospheric Correction\nRemoval of environmental attenuation:Atmospheric scattering and Topographic attenuation\nDOS（Dark object subtraction, PIFs (Psuedo-invariant Features), Py6S, FLAASH, ACORN, QUAC, ATCOR Empirical Line Correction\nradiance reflected above the surface , Atmospheric attenuation\nTop-of-atmosphere reflectance and Surface reflectance\nBiophysical parameters required and the use of spectral signatures across time and space.\n\n\nOrthorectification\nremoving distortions. making the pixels viewed at the nadir (straight down)\nCosine correction, Minnaert correction, Statistical Empirical correction, C Correction (advancing the Cosine)\nradiance, Sun's zenith angle, Sun's incidence angle slope angle (from DEM), slope aspect, solar zenith, solar azimuth\ngeographically aligned and topography removed\nOrthorectification Is a subset of georectification, useful for Precise Georeferencing and High-Precision Measurement and Spatial Analysis\n\n\nRadiometric Calibration\nConverts the digital value (DN) of an image into a physical sense of radiant brightness or reflectance, eliminating the effects of sensors, light sources, etc.\nLλ = Bias + (Gain * DN)\nDN, sensor information, Solar radiation information\nCorrected image\nOften used as a first step in data pre-processing to provide a basis for subsequent atmospheric corrections, etc.\n\n\n\n\n\n\n\n\n\n3.1.2 Joining the data\nIn some cases, we need to merge the images from different regions and times to cover the study area. We had better choose the image from the same day or else we need to standardise data. In remote sensing, it is called “Mosaicking”. This involves complex arithmetic processes: histogram matching algorithm feathering and blending. But it is simple in R just needs some time.\n\nm1 &lt;- terra::mosaic(l83, l84, fun=\"mean\")\n\n\n\n3.1.3 Image Enhancement\nWe choose the appropriate data, process it, and modify it to generate the desired output. The methods help us change the image and produce the output for our purpose. The methods are as follows.\n\n\n\n\n\nEnhancement\npurposes /defination\nmethods\ninput data\nAdvantages\nApplicable scenarios\n\n\n\n\nContrast Enhancement\nImproved visualisation of the image by widening the distribution of pixel values.\nMinimum- Maximum Percentage Linear and Standard Deviation Piecewise Linear Contrast Stretch\nImages with a narrow distributions\nEnhance images contrast and details\npre-processing for subsequent classification and visual interpretation\n\n\nBand Ratioing\nUsing the ratio of reflectance of different bands to highlight specific feature characteristics\ndivision operations (like NDVI, NDBI, tasselled cap etc.)\nMulti-spectral imagery\nEmphasis on relative differences in expression of feature characteristics\nNormalized Burn Ratio, The Normalised Difference Vegetation Index, the Normalized Difference Moisture Index (NDMI)\n\n\nFiltering\nSuppressing noise or highlighting local variations through filter operations.\nLow pass or low frequency (averages the surrounding pixels)/High pass or high frequency-enhance local variations\nOriginal data\nSmooth or sharpen images; extract edge information as required\nFeature extraction, target detection\n\n\nPCA\nDimensionality reduction, removal of inter-band correlations, and extraction of most of the information in the image.\nIn R this is prcomp() from the terra package\nMulti-spectral imagery\nExtracting the main direction of change reduces the computational burden and noise reduction\nData compression, change detection, feature extraction\n\n\nTexture\nmeasuring the relationships between pixel-to-pixel, quantifying roughness to obtain feature characteristics.\nGLCM(Gray Level Co-occurrence Matrix ) second-order texture measures, third-order texture measures\nGrey-scale images or single-band data, windows\nCharacterisation of feature texture, feature classification, target identification\nLand cover classification, target detection, environmental monitoring\n\n\nFusion\nFusion of data from different sensors or different times\nPan-sharpening, Gram-Schmidt, IHS\nMulti-temporal, multi-sensor data\nenhance the details of the images\nHigh resolution data generate, changes detection, data integration",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Corrections and Enhancement</span>"
    ]
  },
  {
    "objectID": "Wk3_Corrections.html#application",
    "href": "Wk3_Corrections.html#application",
    "title": "3  Corrections and Enhancement",
    "section": "3.2 Application",
    "text": "3.2 Application\nTexture analysis is widely used as an image enhancement technique to improve classification accuracy by providing additional spatial information that spectral bands alone cannot capture.\nDengsheng Lu Lu et al. (2012) improved the identification of the Amazonian tropics by using texture fusion with multi-spectral images. By incorporating GLCM texture measures, classification accuracy improved in distinguishing dense forests from secondary vegetation. This demonstrates how texture-based enhancement refines image details and supports better feature extraction. However, some texture measure has high correlations with each other, leading to increased redundant information and possibly affecting classification stability. Although the author finally chose two measures, sifting through all the variables still requires a lot of computation.\nWhich texture-measures combination from GLCM is most suitable for landscape identification? Mryka Hall-Beyer@hall-beyer2017b examines scenarios such as agricultural landscapes. Trying different texture measures in the identification of natural landscapes. Combining texture with the PCA method can filter the most representative texture features and improve computational efficiency. Certain texture features, such as Contrast and Correlation, are particularly effective in capturing spatial variations, while others, like Entropy, provide additional complexity. Different landscapes require different texture measures, as structural complexity varies across environments.\n\nMryka Hall-Beyer (hall-beyer2017buses?) PCA to help with filtering texture measures. Reduced workload in identifying natural landscapes for future research. However, this research only discusses the different groups of texture measures that will bring different influences for the results. It does not mention the influences of resolution and window sizes. Smaller windows are more effective for detecting fine-scale features, while larger windows enhance broader landscape classification, making window selection an important factor in image enhancement. So, in the future, research can focus on the different window sizes for different-resolution images. This will improve the applicability of the texture method at different spatial scales. Now, texture is usually used for natural landscapes like forests. But Ent can detect unnatural edges well. So texture is also important for urban land use identification. For example, Fatemeh Tabib Mahmoudi, Arabsaeedi, and Alavipanah (2019) introduced entropy, contrast, correlation and mean for urban object recognition. In the future there could be more research on applying texture in complex scenarios like cities. Future research should explore integrating texture-based enhancement with other image correction techniques, such as radiometric correction, to improve classification robustness across varying imaging conditions.\n\n\n\nDifferent texture measures visual appearance Source:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Corrections and Enhancement</span>"
    ]
  },
  {
    "objectID": "Wk3_Corrections.html#reflection",
    "href": "Wk3_Corrections.html#reflection",
    "title": "3  Corrections and Enhancement",
    "section": "3.3 Reflection",
    "text": "3.3 Reflection\nThis week we learned Correction, Mosaicking, and image enhancement. We usually comprehend different methods and concepts separately during the learning process. (But the content this week is still hard for me.) In practice, for different bands and spatial resolutions, the effects of the different methods are also different, so it is necessary to combine various methods to realise the purpose of the study. For example, when doing principal component analysis (PCA) on spectral data, the subsequent eigenchannels usually represent indexes like NDVI (vegetation) mentioned in the band ratio.(Hall-Beyer 2017) This suggests that there may be a relationship of complementarity between the different approaches. In addition, in order to get more clearer images and results, researchers often combine different sensor data and methods of analysis.\nThe enhancement method——Texture, currently applies to natural landscape identification such as forests. But I think it can also used in small-scale urban identification because Ent is sensitive to unnatural edges and can be used to identify artificial landscapes. What’s more, compared with popular Spectroscopic methods, Texture methods (e.g. GLCM) focus on the spatial pattern of the image and may reduce the band radiometric errors due to temperature variation caused by the city heat island effect. However, texture can only be used for enlarged objects due to its large computation and the edges of the city are especially complex, which limits its ability to complement other methods in many situations. I believe addressing this limitation could be an important direction for future research, particularly through deep learning-based feature extraction. Automated texture analysis could reduce computational demands and improve classification efficiency, making it more applicable to urban studies. In specific applications, texture features might be beneficial for urban planning, by accurately identifying traffic-congested road sections based on density variations.\n\n\n\n\nHall-Beyer, Mryka. 2017. “Practical Guidelines for Choosing GLCM Textures to Use in Landscape Classification Tasks over a Range of Moderate Spatial Scales.” International Journal of Remote Sensing 38 (5): 1312–38. https://doi.org/10.1080/01431161.2016.1278314.\n\n\nLu, Dengsheng, Mateus Batistella, Guiying Li, Emilio Moran, Scott Hetrick, Corina Da Costa Freitas, Luciano Vieira Dutra, and Sidnei João Siqueira Sant’Anna. 2012. “Land Use/Cover Classification in the Brazilian Amazon Using Satellite Images.” Pesquisa Agropecuária Brasileira 47 (9): 1185–1208. https://doi.org/10.1590/S0100-204X2012000900004.\n\n\nTabib Mahmoudi, Fatemeh, Alireza Arabsaeedi, and Seyed Kazem Alavipanah. 2019. “Feature-Level Fusion of Landsat 8 Data and SAR Texture Images for Urban Land Cover Classification.” Journal of the Indian Society of Remote Sensing 47 (3): 479–85. https://doi.org/10.1007/s12524-018-0914-8.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Corrections and Enhancement</span>"
    ]
  },
  {
    "objectID": "Wk4_Policy.html#international",
    "href": "Wk4_Policy.html#international",
    "title": "Policy",
    "section": "1.1.1 International",
    "text": "1.1.1 International\nThe 2030 Agenda for Sustainable Development also mentioned climate risk reduction in its reports. It is an important part of the 17 Sustainable Development Goals (SDGs). These SDGs build on decades of work by countries and the UN.(“THE 17 GOALS | Sustainable Development,” n.d.)\nSDG11: Make cities and human settlements inclusive, safe, resilient and sustainable\nSDG9: Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation.\nOther historical documents and meetings also mention the challenges caused by flooding, such as the Sendai Framework for Disaster Risk Reduction, which provides guidance for disaster prevention and control, the Guidelines for Reducing Flood Losses launched, and the Hyogo Framework for Action (2005-2015).",
    "crumbs": [
      "Policy"
    ]
  },
  {
    "objectID": "Wk4_Policy.html#metropolitan",
    "href": "Wk4_Policy.html#metropolitan",
    "title": "Policy",
    "section": "1.1.2 Metropolitan",
    "text": "1.1.2 Metropolitan\nA LIVABLE CLIMATE is also a goal In OneNYC 2050, it proposed 4 relative initiatives for building sustainable cities. It advocates strengthening communities, buildings, infrastructure, and the waterfront to be more resilient. (21) To make the city more resilient and safer. The government also collaborated with the U.S. ARMY CORPS OF ENGINEERS to reshape the shoreline of New York. The main measures are divided into three directions: Protecting the neighbourhoud and basic infrastructure, effective management measures and more accurate research and forecasting.\n\n\n\nFlood map(Source:OneNYC 2050)",
    "crumbs": [
      "Policy"
    ]
  },
  {
    "objectID": "Wk4_Policy.html#local",
    "href": "Wk4_Policy.html#local",
    "title": "Policy",
    "section": "1.1.3 Local",
    "text": "1.1.3 Local\nAt the local level, the city will build according to the adopting flood maps which delineate the climate projections. For example, in Lower Manhattan, the city extends the shoreline to the East River, which is 20 meters above the current sea level and set up many seafront parks.\n\n\n\nLocal policy: Lower Manhattan (Source:OneNYC 2050)\n\n\nIn conclusion, current policies are already comprehensive and specific, to explore the effectiveness of these initiatives, I think needs to be made through a data approach.",
    "crumbs": [
      "Policy"
    ]
  },
  {
    "objectID": "Wk4_Policy.html#application",
    "href": "Wk4_Policy.html#application",
    "title": "4  Policy",
    "section": "4.2 Application",
    "text": "4.2 Application\n\n4.2.1 Data and Model\nUsually, flood hazard mapping always includes two parts: the flood models (FL) and natural catastrophe (CAT) models. (Hirpa et al. 2021) Flood models are usually hydrological models based on DEMs, channel locations, stream flow, and precipitation data. The catastrophe models always include land cover data, critical infrastructure, and building footprints extracted from LIDAR data. To support the flood information decision, New York has already drawn a series of maps to inform flood planning and Response. For example, the NYC Stormwater Flood Maps (“NYS Flood Impact Decision Support System - FIDSS,” n.d.) shows the area’s potential flooding scenarios. New York City now makes flooding monitoring with the help of FloodNet Sensors Which are 87 Low-Cost Ultrasonic sensors, which bring more precision results.\n\n\n\nNYC Stormwater Flood Maps (Source: New York City Stormwater Flood Maps)\n\n\nAlthough the hydrological modelling in the local area seems to be progressively improving perfectly. Remote sensing data is still helpful in emergency response and large-scale analyses of flooding. We can get the depth and extent of the flood by Landsat series data. By computing the modified normalized difference water index (NDWI) to do water detection after floodings. (Cohen et al. 2019) We can get flooded areas by comparing it with the Land Cover Dataset. However, Landsat data may be influenced by weather so can not get continuous data. SAR sensors like Sentinel-1 have a higher resolution (10m) and can detect through clouds, so it is more commonly applied to real-time flood monitoring. The flooding analyses estimate the post-to-preflood radar cross-section (RCS) ratio and exclude the double scatterers (DSs) in urban areas.(Mason, Dance, and Cloke 2023) We can also get the depth of the flood by analysing it in relation to the high-resolution DEM data obtained from LIDAR.\n\n\n\nLandsat flooding analysis\n\n\nThe Land Cover Dataset is important in The Social Vulnerability analyses. We can obtain large area land cover monitoring through optical remote sensing like Landsat, Sentinel-2, and MODIS. Using different indexes helps us detect different types of covering.\nNDVI: distinguishes vegetation from non-vegetation.  \n\nNDBI: extracting building area.  \n\nNDWI: extracting body of water.  \nThe DEM Dataset is very important in flooding analyses. It’s very important in remote sensing and hydrological analysis. In the local area, we often use LIDAR to get 1-5m DEM data for more precise analyses. For larger study areas like cities, we can use SAR to get DEM data to simplify the dataset for ease of computation.\n\n\n4.2.2 Workflows\nThis workflow shows how data helps address flood-related policy challenges by supporting risk assessment, planning, and effective response measures.\n\n\n\nWorkflow image",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Policy</span>"
    ]
  },
  {
    "objectID": "Wk4_Policy.html#data-and-model",
    "href": "Wk4_Policy.html#data-and-model",
    "title": "Policy",
    "section": "1.2.1 Data and Model",
    "text": "1.2.1 Data and Model\nUsually, flood hazard mapping always includes two parts: the flood models (FL) and natural catastrophe (CAT) models. () The flood models are usually based on the hydrological modelling with DEM, channel location, river discharge and Precipitation data. The catastrophe models always include Land cover data and critical infrastructure and building footprints extracted from LIDAR data. To support the flood information decision, New York has already drawn a map to inform flood planning and Response. For example, the NYC Stormwater Flood Maps () shows the area’s potential flooding scenarios. New York City now makes flooding monitoring with the help of FloodNet Sensors Which are 87 Low-Cost Ultrasonic sensors, which bring more precision results.\n\n\n\nNYC Stormwater Flood Maps (Source: New York City Stormwater Flood Maps)\n\n\nAlthough the hydrological modelling in the local area seems to be progressively improving perfectly. Remote sensing data is still helpful in emergency response and large-scale analyses of flooding. We can get the depth and extent of the flood by Landsat series data. By computing the modified normalized difference water index (NDWI) to do water detection after floodings. (Cohen et al. 2019) We can get flooded areas by comparing it with the Land Cover Dataset. However, Landsat data may be influenced by weather so can not get continuous data. SAR sensors like Sentinel-1 have a higher resolution (10m) and can detect through clouds, so it is more commonly applied to real-time flood monitoring. The flooding analyses estimate the post-to-preflood radar cross-section (RCS) ratio and exclusion of double scatterers (DSs) in urban areas.(Mason, Dance, and Cloke 2023) We can also get the depth of the flood by analysing it in relation to the high-resolution DEM data obtained from LIDAR.\n\n\n\nLandsat flooding analysis\n\n\nThe Land Cover Dataset is important in The Social Vulnerability analyses. We can obtain large area land cover monitoring through optical remote sensing like Landsat, Sentinel-2, and MODIS. Using different indexes helps us detect different types of covering.\nNDVI: distinguishes vegetation from non-vegetation.  \n\nNDBI: extracting building area.  \n\nNDWI: extracting body of water.  \nThe DEM Dataset is very important in flooding analyses. It’s very important in remote sensing and hydrological analysis. In the local area, we often use LIDAR to get 1-5m DEM data for more precise analyses. For larger study areas like cities, we can use SAR to get DEM data to simplify the dataset for ease of computation.",
    "crumbs": [
      "Policy"
    ]
  },
  {
    "objectID": "Wk4_Policy.html#reflection",
    "href": "Wk4_Policy.html#reflection",
    "title": "4  Policy",
    "section": "4.3 Reflection",
    "text": "4.3 Reflection\nCurrent NYC Flood Maps may be plots based on hydrologic models developed through hydrologic analyses. New York is a coastal city with many important rivers, the analysis will be difficult because of the different hydrological properties of sea and river water. I think using remote sensing data for analysis is simpler. Because water has the same reflective properties, and the analysis is focused on the results rather than the complex hydrological process. It is more suitable for large-scale analysis across the entire city like New York. Moreover, remote sensing enables long-term monitoring of flood impacts, providing valuable data for evaluating policy effectiveness before and after implementation. For example, New York takes many projects to protect Lower Manhattan and plots the project region and location on the map, reflecting zoning planning, which is very specific and highly instructive.Remote sensing data can provide clear changes before and after the implementation of the policy, and provide support for those projects. A Data-Driven Approach to urban planning analyses data from different scopes with different methods leading to more efficient and accurate policy delivery.\nThe methods applied in New York could be adapted for other coastal cities，like Tokyo, Shanghai, which face similar climate challenges. Unlike with radar detectors or precise hydrological data, open-access satellite data (Sentinel-1, Landsat) offers a cost-effective strategy for flood risk analysis and feedback on policy effectiveness, particularly in developing country with limited resources.\n\n\n\n\nCohen, Sagy, Austin Raney, Dinuke Munasinghe, J. Derek Loftis, Andrew Molthan, Jordan Bell, Laura Rogers, et al. 2019. “The Floodwater Depth Estimation Tool (FwDET V2.0) for Improved Remote Sensing Analysis of Coastal Flooding.” Natural Hazards and Earth System Sciences 19 (9): 2053–65. https://doi.org/10.5194/nhess-19-2053-2019.\n\n\nFuleihan, Dean, Dominic Williams, Daniel A Zarrilli, and OneNYC Director. n.d. “The City of New York Mayor Bill de Blasio.”\n\n\nHirpa, Feyera A., Valerio Lorini, Simon J. Dadson, and Peter Salamon. 2021. “Calibration of Global Flood Models.” In, 201–11. American Geophysical Union (AGU). https://doi.org/10.1002/9781119427339.ch11.\n\n\nMason, David C., Sarah L. Dance, and Hannah L. Cloke. 2023. “Toward Improved Urban Flood Detection Using Sentinel-1: Dependence of the Ratio of Post- to Preflood Double Scattering Cross Sections on Building Orientation.” Journal of Applied Remote Sensing 17 (1): 16507. https://doi.org/10.1117/1.JRS.17.016507.\n\n\n“NYS Flood Impact Decision Support System - FIDSS.” n.d. https://sedac.ciesin.columbia.edu/mapping/nysfidss/?page=NYS-FIDSS&views=About.\n\n\n“THE 17 GOALS | Sustainable Development.” n.d. https://sdgs.un.org/goals.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Policy</span>"
    ]
  },
  {
    "objectID": "Wk4_Policy.html#workflows",
    "href": "Wk4_Policy.html#workflows",
    "title": "Policy",
    "section": "1.2.2 Workflows",
    "text": "1.2.2 Workflows\nThe New York government now retrofitted our nearly 1 million buildings and invested more than $20 billion against extreme weather events.\n\n\n\nWorkflow image",
    "crumbs": [
      "Policy"
    ]
  },
  {
    "objectID": "Wk4_Policy.html#summary",
    "href": "Wk4_Policy.html#summary",
    "title": "4  Policy",
    "section": "",
    "text": "4.1.1 International\nThe 2030 Agenda for Sustainable Development also mentioned climate risk reduction in its reports. It is an important part of the 17 Sustainable Development Goals (SDGs). These SDGs build on decades of work by countries and the UN.(“THE 17 GOALS | Sustainable Development,” n.d.)\nSDG11: Make cities and human settlements inclusive, safe, resilient and sustainable\nSDG9: Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation.\nOther historical documents and meetings also mention the challenges caused by flooding, such as the Sendai Framework for Disaster Risk Reduction, which provides guidance for disaster prevention and control, the Guidelines for Reducing Flood Losses launched, and the Hyogo Framework for Action (2005-2015).\n\n\n4.1.2 Metropolitan\nLivable climate is also set as a key goal in OneNYC 2050, it proposed 4 relative initiatives for building sustainable cities. The main measures are divided into three directions: Protecting the neighbourhoud and basic infrastructure, effective management measures and more accurate research and forecasting. It also advocates strengthening communities, buildings, infrastructure, and the waterfront to be more resilient. (21) To make the city more resilient and safer. The government also collaborated with the U.S. ARMY CORPS OF ENGINEERS to reshape the shoreline of New York.\n\n\n\nFlood map(Source:OneNYC 2050)\n\n\n\n\n4.1.3 Local\nAt the local level, the city will build according to the adopting flood maps which delineate the climate projections. For example the East Side Coastal Resiliency (ESCR) Project, in Lower Manhattan, the city extends the shoreline to the East River, which will be 20 meters above the current sea level and set up many seafront parks.\n\n\n\nLocal policy: Lower Manhattan (Source:OneNYC 2050)\n\n\nIn conclusion, current policies are already comprehensive and specific. I think the effectiveness of these initiatives needs to be explored through data methods.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Policy</span>"
    ]
  },
  {
    "objectID": "Wk6_GEE.html",
    "href": "Wk6_GEE.html",
    "title": "5  An introduction to Google Earth Engine",
    "section": "",
    "text": "5.1 Summary\nGoogle Earth Engine(GEE) can access and tackle Geographic information data quickly by using a cloud computing platform. I think GEE has two main benefits.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>An introduction to Google Earth Engine</span>"
    ]
  },
  {
    "objectID": "Wk6_GEE.html#summary",
    "href": "Wk6_GEE.html#summary",
    "title": "5  An introduction to Google Earth Engine",
    "section": "",
    "text": "5.1.1 Benefits\n\nGEE is lazy computing for users compared with traditional data analysis methods like ArcGIS, R and Python. GEE data preprocessing operations, such as reprojection and uniform resolution, can be handled automatically. So, the data in GEE is analysis-ready. It helps users to focus on data analysis and application work.\nGEE can access and deal with large-scale data, especially global remote sensing data. As a cloud-based platform, it is easy for researchers to disseminate their results to others, making the project more reproducible. No need to upload local files to GitHub (which has a size limit, and most remote sensing images can’t be uploaded) to share the results as before.\n\n\n\n\n\n\n\nGithub limitation\n\n\n\n\nThis process uses a pyramid of downsampled tiles, reducing the image to multiple scales until it reaches 256 × 256 pixels per tile. Making the processing more efficient. Here, I use the image and the example of LandSat with 30m resolution to help me understand it.\n\n\n\nResolution Pyramid Source: GEE\n\n\nIn GEE, we can perform lots of processes, like reducing images, linear regression, image enhancements, etc. GEE is mainly programmed with JavaScript; compared with R, the data tackling is more simplified. The table shows the differences between GEE and R.\n\n\n\n\n\nFeature\nR (Local Computing)\nGEE (Cloud Computing)\n\n\n\n\nData Storage\nData is saved on your computer's memory or hard drive.\nData is stored on Google's servers, and your code just refers to it.\n\n\nComputation\nRuns immediately on your computer (eager evaluation).\nRuns only when needed (lazy computation).\n\n\nLooping\nYou can use for loops and apply() to process data.\nYou can't use for loops on ee objects. Instead, you need to use map().\n\n\nVisualization\nNeeds ggplot2 or tmap to draw maps.\nYou can show maps directly with Map.addLayer() on an interactive map.\n\n\nImage data\na stack of rasters\nImage collection\n\n\nData Access\nYou must download data (e.g., Landsat, Sentinel) before using it.\nYou can access data directly from Google Earth Engine’s Data Catalog (no downloading needed).\n\n\nProjection & Resolution\nYou must manually adjust projection and resolution (st_transform(), projectRaster()).\nGEE automatically handles projection and resampling.\n\n\nMachine Learning\nUses external ML libraries like randomForest and xgboost.\nHas built-in ML tools (like Random Forest), but can't use external ML libraries.\n\n\nLimits\nOnly limited by your computer’s power, no API limits.\nFree accounts have limits on computing and storage. Pro accounts are needed for more resources.\n\n\n\n\n\n\n\n\n\n5.1.2 Limitations\nGEE is used in a variety of study areas like global forest change, crop yield estimation, fire recovery, etc. because of its efficient access and computation and the wealth of data types and functions in the library. It can especially show its benefits when it comes to large-scale data or time series analyses. However, it also has limitations. 1. It is influenced by Google’s server resource allocation; Google’s server resources affect request limits, including how long they run and how many can run at the same time.. That may lead to the failure of some complicated computation. 2. It may not work well for operations involving value can be influenced by arbitrarily distant input or data not covered by the GEE library.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>An introduction to Google Earth Engine</span>"
    ]
  },
  {
    "objectID": "Wk6_GEE.html#application",
    "href": "Wk6_GEE.html#application",
    "title": "5  An introduction to Google Earth Engine",
    "section": "5.2 Application",
    "text": "5.2 Application\nI have explored what influence the data from different sensors would have on the analysis of crop yield estimation. This week, I want to focus on the impact of analyses of GEE. I conclude two articles to make comparisons, both of them using a vegetation index to set up a random forest model to predict the crop yield. But one of them used GEE to analyse, the other used traditional methods.\n\nThe role of GEE in data processing\n\nChoudhary et al. Choudhary et al. (2022) are using GEE to remove all errors and get similar weight to tackle the data quickly. These steps help quickly access multi-temporal Sentinel-2 data and reduce the time to deal with data. But GEE data tackling may ignore the unusual weather or special local terrain, which may reduce the prediction accuracy. Hunt et al. Hunt et al. (2019) did data pre-processing manually (like using a buffer to remove gaps from data cleaning), selecting a date image with fewer clouds and using a Sen2Cor processor to correct data, improving the quality of input data, so the model performances are more consistent.\n\nPerformance of random forests in GEE and non-GEE environments\n\nHunt et al. Hunt et al. (2019) using different combinations of data( meteorological, terrain, and soil moisture content) to improve the RF model prediction result in wheat production. Precision of yield estimation at field level (RMSE 0.61 t/ha) is a specific estimation method that can be modified with manual feature adjustment to select model parameters and data input. This approach is more suitable for small-scale, high-precision farm management. Choudhary et al. Choudhary et al. (2022) used GEE for efficient calculation and can quickly complete comparative testing of multiple models (RF, linear regression, decision trees). However, the model accuracy is (RMSE 0.40 t/ha) lower than traditional methods. The advantage of GEE is that it is suitable for large-scale agricultural monitoring. It is easy to analyse vegetation dynamics across seasons and over long periods of time using GEE. GEE can automate data handling processes, although it limits variable screening and manual model simulations affect accuracy, but it is suitable for large scale learning.\n\n\n\n\n\nLocal Computing Source: Hunt et al. (2019)\n\n\n\n\n\n\n\nGEE Cloud Platform Source: Choudhary et al. (2022)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>An introduction to Google Earth Engine</span>"
    ]
  },
  {
    "objectID": "Wk6_GEE.html#reflection",
    "href": "Wk6_GEE.html#reflection",
    "title": "5  An introduction to Google Earth Engine",
    "section": "5.3 Reflection",
    "text": "5.3 Reflection\nAfter comparing the above two articles, according to the features of the plant, I think it is more appropriate to exchange the tools used by the article. Wheat is mostly grown in the plains with larger areas per farmland, which has been put into mechanised production on a large scale. So, it is more suitable for larger-scale predictions using GEE. GEE can be monitored across multiple growing seasons, providing data for large-scale crop yield estimation, supply chain regulation and reduction monitoring. Field crops such as corn and soybeans are also well-suited for GEE analysis.\nLocal calculation can use high resolution commercial remotely sensed data (like quick bird) is more suitable for rice. Because rice is a crop with small field sizes and complex growing practices (e.g. terraced planting, mixed cropping) that requires careful management. Specific local calculation can combine local climate and terrain data to classify and analyse more accurately which support the intensive farming needs such as pest and disease monitoring, soil and water conservation, etc. GEE is limited by its resolution, making it difficult to meet the accuracy needs However, high-resolution remote sensing data is expensive, now is only available using in Japan and Korea. For other rice-growing Asian countries it is hard to promote. So, GEE will help these country access to remote-sensing data with low-cost, extensive crop monitoring and food security in those place. In future, the process depends not only on data quality and computational power but also can combine with crop growth characteristics, agricultural production patterns and economic viability.\n\n\n\n\nChoudhary, K., W. Shi, Y. Dong, and R. Paringer. 2022. “Random Forest for Rice Yield Mapping and Prediction Using Sentinel-2 Data with Google Earth Engine.” Advances in Space Research 70 (8): 2443–57. https://doi.org/10.1016/j.asr.2022.06.073.\n\n\nHunt, Merryn L., George Alan Blackburn, Luis Carrasco, John W. Redhead, and Clare S. Rowland. 2019. “High Resolution Wheat Yield Mapping Using Sentinel-2.” Remote Sensing of Environment 233 (November): 111410. https://doi.org/10.1016/j.rse.2019.111410.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>An introduction to Google Earth Engine</span>"
    ]
  },
  {
    "objectID": "Wk7_Clas1.html",
    "href": "Wk7_Clas1.html",
    "title": "6  Classification Ⅰ",
    "section": "",
    "text": "6.1 Summary\nThis week’s learning covered image classification. This is based on the pattern recognition or machine learning to turn every pixel in the image into one of classification. Image classification is applied in many areas like agriculture monitoring, air pollution, forest monitoring and urban green spaces assessment.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification Ⅰ</span>"
    ]
  },
  {
    "objectID": "Wk7_Clas1.html#conclusion-of-the-process-of-analysing-remotely-sensed-images",
    "href": "Wk7_Clas1.html#conclusion-of-the-process-of-analysing-remotely-sensed-images",
    "title": "6  Classification Ⅰ",
    "section": "6.2 Conclusion of the process of analysing remotely sensed images",
    "text": "6.2 Conclusion of the process of analysing remotely sensed images\nCombining the previous studies and articles, I think the analysis of remotely sensed images includes: 1. Data obtained – 2 pre-processed- 3. image enhancement-4. Factor choosing -5. Predict modelling(classification/regression)-6. Result assessment. Image classification belongs to the fifth step, which directly affects the final research results.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification Ⅰ</span>"
    ]
  },
  {
    "objectID": "Wk7_Clas1.html#approaches-to-image-classification",
    "href": "Wk7_Clas1.html#approaches-to-image-classification",
    "title": "6  Classification Ⅰ",
    "section": "6.3 Approaches to image classification",
    "text": "6.3 Approaches to image classification\n\n6.3.1 Unsupervised classification\nThese methods are applied to identify data with unknown land cover classes. Data were labelled into different categories using ML methods like clustering (k-means, DB-scan) based on spectral features. This reduces the influence of the human factor. The initial result of unsupervised classification may have mixed categories, so it always needs downscaling methods like PCA to optimise.\n\n\n\nUnsupervised classification Source: [GISgeography](https://gisgeography.com/image-classification-techniques-remote-sensing/)\n\n\n\n\n6.3.2 Supervised classification\nThese methods are the Classifier learns pattern in the data and puts the labels onto new data. It includes Parametric methods and Non-parametric methods. Recent studies tend to use machine learning/expert systems or spectral hybrid analyses. The steps in supervised classification include class definition, pre-processing, training, pixel assignment, and accuracy assessment.\n\n\n\nSupervised classification Source: [GISgeography](https://gisgeography.com/image-classification-techniques-remote-sensing/)\n\n\n\n6.3.2.1 Parametric methods\nThey are based on the assumption that the data follow a certain distribution pattern (normal distribution). These are traditional methods. The advantage is the clarity of the calculation process. However, they have strict requirements on data distribution and are only applied to small data volumes and simple solutions.\n\n\n6.3.2.2 Non-parametric methods\nThey can apply to complex calculation processes, do not rely on the data distribution and are very popular now. They are computation-heavy and require a large amount of data to train the model.\nML methods have high accuracy but are poor in interpretation. It is difficult to describe the reasons for classification. Overfitting is also a main problem in ML. So, in the process of using an ML model, it is important to choose a suitable model using reasonable hyperparameters. When building training and test sets, it is necessary to ensure the classification’s generalisation ability. For example, we tried different ways to choose training and test sets. And get different accuracy.\n\n// Sorting by polygons\nvar withRandom = polygons.randomColumn('random');\nvar split = 0.7;\nvar trainingPartition = withRandom.filter(ee.Filter.lt('random', split));\nvar testingPartition = withRandom.filter(ee.Filter.gte('random', split));\n\nprint(trainingPartition, \"train\")\nprint(testingPartition, \"test\")\n\n\n// Sorting by pixel\nvar pixel_number= 1000;\n\nvar urban_low_points=ee.FeatureCollection.randomPoints(urban_low, pixel_number).map(function(i){\n  return i.set({'class': 1})})\n  \nvar water_points=ee.FeatureCollection.randomPoints(water, pixel_number).map(function(i){\n  return i.set({'class': 2})})\n  \nvar urban_high_points=ee.FeatureCollection.randomPoints(urban_high, pixel_number).map(function(i){\n  return i.set({'class': 3})})\n\nvar grass_points=ee.FeatureCollection.randomPoints(grass, pixel_number).map(function(i){\n  return i.set({'class': 4})})\n\nvar bare_earth_points=ee.FeatureCollection.randomPoints(bare_earth, pixel_number).map(function(i){\n  return i.set({'class': 5})})\n\nvar forest_points=ee.FeatureCollection.randomPoints(forest, pixel_number).map(function(i){\n  return i.set({'class': 6})})\n\nvar point_sample=ee.FeatureCollection([urban_low_points,\n                                  water_points,\n                                  urban_high_points,\n                                  grass_points,\n                                  bare_earth_points,\n                                  forest_points])\n                                  .flatten()\n                                  .randomColumn();\n                                    \nvar split=0.7\nvar training_sample = point_sample.filter(ee.Filter.lt('random', split));\nvar validation_sample = point_sample.filter(ee.Filter.gte('random', split));",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification Ⅰ</span>"
    ]
  },
  {
    "objectID": "Wk7_Clas1.html#application",
    "href": "Wk7_Clas1.html#application",
    "title": "6  Classification Ⅰ",
    "section": "6.4 Application",
    "text": "6.4 Application\nRemotely sensing image classification is an important method of feature identity. Asbestos-containing material (ACM) roofs were widely used in many residential buildings because of their good heat preservation properties. However, airborne asbestos can cause serious damage to human health, especially to the respiratory system. Recently, Italy and Poland have introduced relative policies: Progressive removal of ACM roofs. Remote sensing imagery can help with the implementation of policies quickly and effectively. Using high-resolution images (like Sentinel-2) can help the government remotely monitor the distribution of ABM roofs. This reduces the cost of surveys in the field.\n\n\n\nThe general progress of the studies of ACM roof mapping Source:Abbasi et al. (2022)\n\n\nBecause the material of roofs has similar spectral properties. Therefore, identification ACM need higher resolution and spectral requirements. Hyperspectral imaging (HIS) has been widely used in studies of ACM roof mapping. There are lots of pixel-based image analysis (PBIA) approaches being used in studies like spectral angle mapper, convolutional neural networks (CNN) and random forest (RF). Accuracy varies from method to method. Convolutional neural networks (CNN) seem to have the highest accuracy.\n\n\n\nThe summary of PBIA for ACM studies Source:Abbasi et al. (2022)\n\n\nMaurizio Tommasini analysed WorldView-3 satellite imagery with Pansharpening and used the Roof Classify Plugin in QGIS. The author chose RF in machine learning to do classification. The method can detect 90% of ACM roofs. This is a Supervised classification that needs preparation classified files and images with labels. But PBIA has its limitation: due to spectral variations, pixels on the same roof may be misclassified. This leads to a loss of accuracy.\n\n\n\nA classified image using RF Source:Tommasini, Bacciottini, and Gherardelli (2019)\n\n\nDeep learning is also applied to identification. It relies on its ability to handle large amounts of data well. DL can be analysed more accurately in the studies of ACM roof mapping, which demands high-resolution data. CNNs can automatically learn complex spatial features through hierarchical convolutional operations. Małgorzata Krówczyńska using CNNs to training the data. The OA of the training and the validation of the training were 93% and 86%. It performs better than RF and SAM. CNNs can ignore the influences from the neighbourhood pixel. So, it is more suitable for studies about urban roof cover identification. In the future, we can use multi-temporal remote sensing data to monitor roof abatement changing trends. It helps us learn about the effectiveness of the implementation of government policies.\n\n\n\nThe result of the classification of ACM foofs using CNNs Source:Krówczyńska et al. (2020)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification Ⅰ</span>"
    ]
  },
  {
    "objectID": "Wk7_Clas1.html#reflection",
    "href": "Wk7_Clas1.html#reflection",
    "title": "6  Classification Ⅰ",
    "section": "6.5 Reflection",
    "text": "6.5 Reflection\nThis week, I learned the application of ML in remote sensing image classification. In particular, deep learning methods have a high accuracy for feature recognition. Through remote sensing image classification, the land cover can be identified easily. It can be used in roof cover identification to reflect the results of relative urban renewal policies like the removal of ACM roofs. It can help monitor policy implementation. Remote sensing images are also valuable for disaster assessment, such as evaluating infrastructure damage caused by wars. However, it is often limited by data accessibility and computational cost. Mueller et al. (2021)\nHowever, to identify the building-level images, we need high-resolution and a huge amount of computation, which may cost a lot of money. That is opposite to the low-cost and large-scale monitoring, which are the original goals to use remote sensing data sets. Therefore, Future research should focus on improving the classification accuracy of open-access and low-resolution datasets. Using methods like super-resolution techniques or image enhancement (like texture) can enhance low-resolution satellite images, the SAR data could provide complementary information. Through these methods, the remote sensing data can be used in more areas to monitor the policy results. The lower costs and easy methods to get data will enable more developing countries to use remote sensing for policy monitoring and disaster recovery. This can help with expanding the global applicability of remote sensing image classifications.\n\n\n\n\nAbbasi, Mohammad, Sherif Mostafa, Abel Silva Vieira, Nicholas Patorniti, and Rodney A. Stewart. 2022. “Mapping Roofing with Asbestos-Containing Material by Using Remote Sensing Imagery and Machine Learning-Based Image Classification: A State-of-the-Art Review.” Sustainability 14 (13). https://doi.org/10.3390/su14138068.\n\n\nKrówczyńska, Małgorzata, Edwin Raczko, Natalia Staniszewska, and Ewa Wilk. 2020. “Asbestoscement Roofing Identification Using Remote Sensing and Convolutional Neural Networks (CNNs).” Remote Sensing 12 (3): 408. https://doi.org/10.3390/rs12030408.\n\n\nMueller, Hannes, Andre Groeger, Jonathan Hersh, Andrea Matranga, and Joan Serrat. 2021. “Monitoring War Destruction from Space Using Machine Learning.” Proceedings of the National Academy of Sciences of the United States of America 118 (23): 1–9. https://www.jstor.org/stable/27040853.\n\n\nTommasini, Maurizio, Alessandro Bacciottini, and Monica Gherardelli. 2019. “A QGIS Tool for Automatically Identifying Asbestos Roofing.” ISPRS International Journal of Geo-Information 8 (3). https://doi.org/10.3390/ijgi8030131.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification Ⅰ</span>"
    ]
  },
  {
    "objectID": "Wk8_Clas2.html",
    "href": "Wk8_Clas2.html",
    "title": "7  Classification Ⅱ",
    "section": "",
    "text": "7.1 Summary\nThis week, we learned about land cover classifications and accuracy. The first part is about advanced classification methods, the second part is about Error Matrix, and the third part is about cross-validation.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification Ⅱ</span>"
    ]
  },
  {
    "objectID": "Wk8_Clas2.html#summary",
    "href": "Wk8_Clas2.html#summary",
    "title": "7  Classification Ⅱ",
    "section": "",
    "text": "7.1.1 Advanced classification methods\nClassification methods include Pixel-Based Image Analysis and Object-Based Image Analysis. PBIA relies on the spectral properties of pixels for classification. “OBIA groups pixels into representative vector shapes with size and geometry” (“Image Classification Techniques in Remote Sensing,” n.d.), making classification results more spatially continuous. Sub-pixel analysis is a branch of PBIA, and it is used to solve the problem of pixel size that is bigger than the target object. I interpret the OBIA as a jigsaw puzzle that is in a high-resolution image, objects are composed of multiple pixels. Sub-pixel analysis is like a kind of colour palette that is in a low-resolution image, pixels are composed of multiple objects. Usually, OBIA is suitable for the high-quality classification of high-resolution images, and sub-pixel analysis applies to the mixed-pixel problem of low-resolution data. The following table will list how they are used.\n\n\n\n\n\nCriteria\nObject-Based Image Analysis (OBIA)\nSpectral Mixture Analysis (SMA)\n\n\n\n\nBasic Unit\nObject (multiple pixels)\nMixed pixel (one pixel contains multiple land cover types)\n\n\nResolution\nHigh-resolution (UAV, WorldView)\nLow to medium resolution (MODIS, Landsat)\n\n\nClassification\nMachine learning (SVM, Decision Tree, KNN, Random Forest)\nSpectral unmixing (LSU, MESMA, V-I-S model)\n\n\nFeatures Used\nSpectral, spatial, shape-related (length-width ratio, edge length, compactness)\nSpectral only\n\n\nMain Methods\nSegmentation: SLIC, Watershed, Multi-resolution Segmentation\nUnmixing: Linear Spectral Unmixing (LSU), MESMA\n\n\nNA\nClassification: SVM, Random Forest, Decision Tree, KNN\nLand Cover Fractions: V-I-S model\n\n\nApplications\nUrban classification, land cover mapping\nMixed land cover analysis, vegetation fraction estimation\n\n\nComplexity\nHigh (segmentation + classification)\nLow (spectral analysis only)\n\n\nAdvantages\nHigh accuracy, noise reduction\nEstimates sub-pixel land cover proportions\n\n\nLimitations\nRequires segmentation, preprocessing\nSensitive to endmember selection\n\n\n\n\n\n\n\n\n\n\nPBIA vs OBIA Source: [GISgeography](https://gisgeography.com/image-classification-techniques-remote-sensing/)\n\n\n\n\n7.1.2 Accuracy\nBased on the error matrix, there are many indexes and ways to measure the accuracy of the classification results. I think this part can be understood in steps. Basically, the accuracy is calculated by the error matrix with the overall accuracy, user accuracy, and producer accuracy. These methods can measure most situations but may be incorrect on data with unevenly distributed categories. The Kappa coefficient can solve this problem to some extent. But when there is an extreme imbalance in the distribution of a category, it will still overestimate model performance. (Comber et al. 2012) F1 and ROC curves can also help with measuring model accuracy, ROC is more accurate.\n\n\n\n\n\nTerm\nDefinition\nFormula / Key Info\n\n\n\n\nOverall Accuracy (OA)\nPercentage of correctly classified samples among all samples.\nTP + TN / (TP + FP + FN + TN)\n\n\nProducer’s Accuracy (PA)\nProbability that a real land cover class is correctly classified.\nTP / (TP + FN)\n\n\nUser’s Accuracy (UA)\nProbability that a predicted class is actually correct.\nTP / (TP + FP)\n\n\nOmission Error\nHow often a real class is missed.\nFN / (TP + FN)\n\n\nCommission Error\nHow often a predicted class is incorrect.\nFP / (TP + FP)\n\n\nKappa Coefficient (Kappa, κ)\nMeasures agreement between predicted and actual classes, adjusting for chance agreement.\nκ = (p_o - p_e) / (1 - p_e), where p_o is observed accuracy, p_e is expected accuracy.\n\n\nF1-Score\nHarmonic mean of precision and recall, balancing both metrics.\n2 × (Precision × Recall) / (Precision + Recall)\n\n\nConfusion Matrix\nA table showing actual vs. predicted classifications, used to compute accuracy metrics.\nContains TP, FP, FN, TN values.\n\n\nTrue Positive (TP)\nCorrectly classified positive cases.\nExample: Correctly classified urban areas.\n\n\nFalse Positive (FP)\nIncorrectly classified positive cases.\nExample: A forest misclassified as urban.\n\n\nTrue Negative (TN)\nCorrectly classified negative cases.\nExample: Correctly classified non-urban areas.\n\n\nFalse Negative (FN)\nIncorrectly classified negative cases.\nExample: An urban area misclassified as non-urban.\n\n\nCross Validation (CV)\nSplitting data into training and validation sets to evaluate model performance.\nCommon methods: K-Fold, Leave-One-Out (LOO), Spatial CV.\n\n\nK-Fold Cross Validation\nDivides data into K groups, using each fold as a test set once.\nCommon choices: 5-Fold, 10-Fold.\n\n\nSpatial Cross Validation\nEnsures training and test data are spatially separated to avoid overfitting.\nUses spatial partitioning (e.g., K-Means, Voronoi).\n\n\n\n\n\n\n\n\n\n7.1.3 Cross-validation\nImage classification can be viewed as a complete process of machine learning, so it can use cross-validation in ML. Data is divided into a training set and a test set to test the accuracy of the model. However, remote sensing is specific because neighbouring pixels may have a spatial correlation. So, using regular methods will give the false belief that the model is too accurate.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification Ⅱ</span>"
    ]
  },
  {
    "objectID": "Wk8_Clas2.html#application",
    "href": "Wk8_Clas2.html#application",
    "title": "7  Classification Ⅱ",
    "section": "7.2 Application",
    "text": "7.2 Application\nThe differences between PBIA and OBIA arise from two articles on the classification of agricultural landscapes. Duro@duro2012a utilised a moderate-resolution image (10m) to identify Canada’s agricultural landscape, which includes cropland, grassland, soil, and wetland. A comparison of the results of PBIA and OBIA was conducted using three classification methods (DT, SVM, and RF), showing no significant differences (p &gt; 0.05). Wetlands are an exception; OBIA performs more accurately than PBIA in this regard, which may be attributed to the unique characteristics of wetlands. Wetlands comprise water bodies, vegetation, bare ground, and organic sediments. These components create a single pixel of spectral information that can be influenced by various factors. Additionally, the gradual blurring and complex boundaries of wetlands complicate their identification through pixel analysis. OBIA can integrate neighbouring pixels, thus providing a more generalised visual appearance and a more continuous description of land cover.\n\n\n\n\n\nPBIA RF Source: Duro, Franklin, and Dubé (2012)\n\n\n\n\n\n\n\nOBIA RF Source: Duro, Franklin, and Dubé (2012)\n\n\n\n\nCastillejo-González@castillejo-gonzález2009 concluded that OBIA achieved higher accuracy than PBIA in identifying agricultural landscapes. The reasons are as follows: First, the resolution of the image is higher (3m). Second, the agricultural landscapes are more carefully categorised. It not only includes bare soil and cropland but also divides cropland into olive orchards, vineyards and sunflower fields. So, it has higher requirements for classification accuracy. Third, the classification methods employed by the author are traditional approaches, including MD, MC and SAM, unlike Duro, who used machine learning methods. Finally, for measuring accuracy, Elsevier used the kappa coefficients, which may lead to some errors in the results due to the uneven distribution of the categories. However, Dennis used spatial cross-validation k-fold, which is more suitable for remote sensing data classification.\n\n\n\nPBIA vs OBIA :Castillejo-González et al. (2009)\n\n\nIn summary, for most open-source data today, PBIA and OBIA using machine learning will produce similar results. However. CNNs are able to automatically learn more complex spatial features through convolutional operations and the information from surrounding pixels. That advanced algorithm fills the gap between PBIA and OBIA. However, OBIA requires more computing power and time because of image segmentation. I think that in general situations, PBIA using ML methods is enough for most image classification missions with high efficiency. OBIA can be used for heterogeneous landscapes with unclear boundaries, like wetlands. Now, there are many studies that combine PBIA and OBIA together.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification Ⅱ</span>"
    ]
  },
  {
    "objectID": "Wk8_Clas2.html#reflection",
    "href": "Wk8_Clas2.html#reflection",
    "title": "7  Classification Ⅱ",
    "section": "7.3 Reflection",
    "text": "7.3 Reflection\nComparing PBIA and OBIA, I have not only observed that different methods produce different analyses for different resolutions of data but also recognized the importance of balancing computational cost and accuracy. Machine learning methods are especially used for image classification. The balance between effectiveness and accuracy is also important for Policy implementation. For example, is OBIA combined with simple machine learning methods, such as Random Forest (RF), better than PBIA combined with deep learning (CNN) for the same object classification task? This question confuses me. What is the appropriate trade-off between the cost and accuracy of classification of these two methods? In the future, There may be more ways to effectively combine PBIA and OBIA. These methods are now widely used in environmental monitoring, urban planning, and agricultural mapping. Moreover, because the authors use different methods to measure the accuracy of classification, their results need to be further verified. Did the author use cross-validation or Kappa? What is the specific error matrix for the classification model? Does the author consider spatial autocorrelation? The distribution of classifications also requires attention; do the prediction results have an uneven distribution? We should be careful when we read articles. The result in a high OA does not mean the categorisation is good.\nI find this content really interesting because it emphasizes the limitations of classification techniques in applications, perhaps through hybrid models that combine their strengths. In the future, improving accuracy measuring context more effectively could enhance the reliability of classification results. This reflection has made me more aware of the complexity of classification methodologies and the need for careful evaluation when applying them to geospatial problems, particularly in fields where policy decisions rely on accurate spatial analysis.\n\n\n\n\nCastillejo-González, Isabel Luisa, Francisca López-Granados, Alfonso García-Ferrer, José Manuel Peña-Barragán, Montserrat Jurado-Expósito, Manuel Sánchez De La Orden, and María González-Audicana. 2009. “Object- and Pixel-Based Analysis for Mapping Crops and Their Agro-Environmental Associated Measures Using QuickBird Imagery.” Computers and Electronics in Agriculture 68 (2): 207–15. https://doi.org/10.1016/j.compag.2009.06.004.\n\n\nComber, Alexis, Peter Fisher, Chris Brunsdon, and Abdulhakim Khmag. 2012. “Spatial analysis of remote sensing image classification accuracy遥感影像分类精度的空间分析.” Remote Sensing of Environment 127 (December): 237–46. https://doi.org/10.1016/j.rse.2012.09.005.\n\n\nDuro, Dennis C., Steven E. Franklin, and Monique G. Dubé. 2012. “A Comparison of Pixel-Based and Object-Based Image Analysis with Selected Machine Learning Algorithms for the Classification of Agricultural Landscapes Using SPOT-5 HRG Imagery.” Remote Sensing of Environment 118 (March): 259–72. https://doi.org/10.1016/j.rse.2011.11.020.\n\n\n“Image Classification Techniques in Remote Sensing.” n.d. https://gisgeography.com/image-classification-techniques-remote-sensing/.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification Ⅱ</span>"
    ]
  }
]