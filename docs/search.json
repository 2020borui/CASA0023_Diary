[
  {
    "objectID": "Wk1_Introduction_to_remote_sensing.html",
    "href": "Wk1_Introduction_to_remote_sensing.html",
    "title": "1  Introduction to remote sensing",
    "section": "",
    "text": "1.1 Summary\nThis week’s learning content can be divided into two parts: The first part is the concept of remote sensing and the process of gaining the data. Especially, discussing how the interaction between Electromagnetic radiation and Earth’s surface (reflecting, absorbing and scattering) influences the data quality. There are different impacts on electromagnetic radiation with different wavelengths, so it’s important to choose suitable bands to analyse. Moreover, the weather especially the clouds will influence on capturing data. SAR can go through the clouds to reduce the impacts.\nThe second parts introduce the type of remote data, and how to simply deal with data by plotting spectral signatures. In this practical we use snap and r to deal with Landsat and Sentinel data. The main difference between remote data is from four resolutions: spatial, spectral, temporal and radiometric.\nThe steps are:\nTasseled Caps",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to remote sensing</span>"
    ]
  },
  {
    "objectID": "Wk1_Introduction_to_remote_sensing.html#summary",
    "href": "Wk1_Introduction_to_remote_sensing.html#summary",
    "title": "1  Introduction to remote sensing",
    "section": "",
    "text": "remote sensing process Source: Remote Sensing for Dummies\n\n\n\n\n\n\nDownloading data with less cloud cover, choosing different band combinations for different visuals\n\n\nMasking the study area and resampling the data in one spatial resolution using the nearest neighbour. Choosing down-scaling to avoid errors when calculating. The nearest neighbour is more effective and faster than bilinear or cubic convolution. But there will be abrupt changes at the boundary leading to large errors. When deciding to resample the resolution should not change too much which will influence the accuracy of the result.\n\n\nSelect POIs, and create vector data containers for different land cover.\n\n\nDraw vector, export the Geo-tiff and shapefile load the file in R studio, and extract the value of tiff by vector.\n\n\nPlotting the Spectral signatures and comparing the difference of two sensors with the same band and same land use by t-test. The value of Landsat is much greater than that of Sentinel.\n\n\n\n\n\n\n\n\n\n\n\nLandsat Spectral signatures\n\n\n\n\n\n\n\nSentinel Spectral signatures",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to remote sensing</span>"
    ]
  },
  {
    "objectID": "Wk1_Introduction_to_remote_sensing.html#applications",
    "href": "Wk1_Introduction_to_remote_sensing.html#applications",
    "title": "1  Introduction to remote sensing",
    "section": "1.2 Applications",
    "text": "1.2 Applications\nI’m interested in remote sensing in vegetation monitoring. The main difference between the two sensors Landsat and Sentinel is in the various resolutions on spatial, spectral and temporal. By comparing two articles about machine learning used in vegetation monitoring, I observed that two types of data perform differently in different tasks. Sentinel data performs more accurately in predicting by regression on a small scale, the Landsat data is more accurate in classifying vegetation types over large areas.\nIt may be due to the following reasons: Sentinel has a red edge band (B5, B6 and B7) that is more sensitive to monitoring vegetation. It also has a higher resolution of around 10M and 20M, so it has a more accurate prediction of vegetation characteristics such as trunk height for forest variables. (Astola et al. 2019) Sentinel has a 5-day return cycle, so, it captures short-term changes in vegetation well, but it will be complicated to handle large ranges of data. As a new sensor from 2015, it may be a lack of data build-up.\n\n\n\nModel accuracy for different data sources Source: Ram C. Sharma’s article\n\n\nTo my surprise, Landsat data are more accurate in studies related to the classification of vegetation types by setting a random forest model. (Sharma, Hara, and Tateishi 2017)Although Landsat has a 16-day return cycle, due to the length of time covered it has rich datasets and it has SWIR and thermal infrared bands, both of which can provide help for the study of large-scale classification. At the end of the article, the author got a more precise classification by combination of the two types of data.\nHow can these methods be applied to real life? Scientists found that near-infrared radiation can observe the health of vegetation. It is based on the principle that the health status of plants will influence the plant’s spectrum of both absorption and reflection. The U.S. Department of Agriculture uses Landsat and the U.S. Geological Survey to forecast agricultural productivity in each growing season. (Blickensdörfer et al. 2022)According to the comparison above, I think the result will be better to use Landsat data for large-scale farmland species identification, and then use Sentinel data to monitor small-scale changes in the growing season of a single crop.\n\n\n\nThe difference between Sentinel-2 and Landsat 8\n\n\nComparison Factors\nSentinel-2\nLandsat 8\n\n\n\n\nTemporal Coverage\n~73 times/year (high frequency)\n~23 times/year (low frequency)\n\n\nSpatial Resolution\n10 m (selected bands), 20 m (red edge)\n30 m (except 15 m panchromatic)\n\n\nBand Configuration\nVisible, NIR, SWIR, red edge (no thermal infrared)\nVisible, NIR, SWIR, thermal infrared\n\n\nClassification Performance\nMulti-temporal features enhance vegetation health classification\nLong-term data improves stability, versatile band coverage\n\n\nAdvantages\nSuperior for small-scale, short-term changes (e.g., vegetation health)\nSuperior for large-scale, long-term changes (e.g., vegetation types)\n\n\nApplications\nBetter for specific feature prediction\nBetter for large-scale classification",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to remote sensing</span>"
    ]
  },
  {
    "objectID": "Wk1_Introduction_to_remote_sensing.html#reflection",
    "href": "Wk1_Introduction_to_remote_sensing.html#reflection",
    "title": "1  Introduction to remote sensing",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection\nThrough this week I have learnt about the principles of remote sensing and a deep understanding of the advantages and application of Sentinel-2 and Landsat 8 data. Considering that radiation is an important element in remote sensing, I am wondering how climate change affects remote sensing vegetation monitoring. With global warming and the greenhouse effect, will the increased thermal radiation in the atmosphere have impacts on remote sensing results? Will the increased plant transpiration due to warming affect the absorption of red light and the reflection of near-infrared light? Can remote sensing capture the slight changes? Does the use of thermal infrared bands in vegetation health monitoring need to be adapted due to global warming? These issues need to be addressed step by step in the future.\nAlthough there are massive studies about using and comparing Landsat and Sentinel-2 data on vegetation research, I am interested in how to innovate under the background of climate change. Could we build a model to predict the influence of climate on vegetation growth? Based on that we can predict how vegetation in a region or globally will be distributed over the next 50 years based on current climate change. It’s a big help to our food security and species conservation. Moreover, through this week’s studying, I also learned how to choose data with suitable resolution and choose a band which is more suitable for specific tasks. I think the ability to choose suitable data sets and methods is important in future study and learning.\n\n\n\n\nAstola, Heikki, Tuomas Häme, Laura Sirro, Matthieu Molinier, and Jorma Kilpi. 2019. “Comparison of Sentinel-2 and Landsat 8 Imagery for Forest Variable Prediction in Boreal Region.” Remote Sensing of Environment 223 (March): 257–73. https://doi.org/10.1016/j.rse.2019.01.019.\n\n\nBlickensdörfer, Lukas, Marcel Schwieder, Dirk Pflugmacher, Claas Nendel, Stefan Erasmi, and Patrick Hostert. 2022. “Mapping of Crop Types and Crop Sequences with Combined Time Series of Sentinel-1, Sentinel-2 and Landsat 8 Data for Germany.” Remote Sensing of Environment 269 (February): 112831. https://doi.org/10.1016/j.rse.2021.112831.\n\n\nSharma, Ram, Keitarou Hara, and Ryutaro Tateishi. 2017. “High-Resolution Vegetation Mapping in Japan by Combining Sentinel-2 and Landsat 8 Based Multi-Temporal Datasets Through Machine Learning and Cross-Validation Approach.” Land 6 (3): 50. https://doi.org/10.3390/land6030050.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to remote sensing</span>"
    ]
  },
  {
    "objectID": "Wk3_Corrections.html",
    "href": "Wk3_Corrections.html",
    "title": "3  Corrections and Enhancements",
    "section": "",
    "text": "3.1 Summary\nThe lecture includes two parts: Correcting data and Accessing data. This is preparation work for conducting data research.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Corrections and Enhancements</span>"
    ]
  },
  {
    "objectID": "Wk4_Policy.html",
    "href": "Wk4_Policy.html",
    "title": "4  Policy",
    "section": "",
    "text": "4.1 Summary\nNow, we are against the challenges of a changing climate. Climate change has led to an increased risk of natural disasters, such as flooding, which pose significant challenges to the safety of urban residents and urban infrastructure. According to the projections from NPCC, at the end of the century, the frequency of 100-year floods will be up to 3.6%.\nNew York as a famous coastal city has hundreds of miles of waterfront and commercial harbours. It will face more emergency crises from flooding. By the 2050s, the sea levels rise in New York are twice the global average, nearly 1 million residents will be influenced by the coastal floodings, some commercial regions will be flooded without protective measures. (Fuleihan et al., n.d.) New York has already take many measure to face this emergency, for example, construction of marine parks, widening and elevation of the coastline, enhancement of flood prevention infrastructure, etc.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Policy</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "1  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Abbasi, Mohammad, Sherif Mostafa, Abel Silva Vieira, Nicholas Patorniti,\nand Rodney A. Stewart. 2022. “Mapping Roofing with\nAsbestos-Containing Material by Using Remote Sensing Imagery and Machine\nLearning-Based Image Classification: A State-of-the-Art Review.”\nSustainability 14 (13). https://doi.org/10.3390/su14138068.\n\n\nAimaiti, Yusupujiang, Christina Sanon, Magaly Koch, Laurie G. Baise, and\nBabak Moaveni. 2022. “War Related Building Damage Assessment in\nKyiv, Ukraine, Using Sentinel-1 Radar and Sentinel-2 Optical\nImages.” Remote Sensing 14 (24): 6239. https://doi.org/10.3390/rs14246239.\n\n\nAstola, Heikki, Tuomas Häme, Laura Sirro, Matthieu Molinier, and Jorma\nKilpi. 2019. “Comparison of Sentinel-2 and Landsat 8 Imagery for\nForest Variable Prediction in Boreal Region.” Remote Sensing\nof Environment 223 (March): 257–73. https://doi.org/10.1016/j.rse.2019.01.019.\n\n\nBlickensdörfer, Lukas, Marcel Schwieder, Dirk Pflugmacher, Claas Nendel,\nStefan Erasmi, and Patrick Hostert. 2022. “Mapping of Crop Types\nand Crop Sequences with Combined Time Series of Sentinel-1, Sentinel-2\nand Landsat 8 Data for Germany.” Remote Sensing of\nEnvironment 269 (February): 112831. https://doi.org/10.1016/j.rse.2021.112831.\n\n\nCastillejo-González, Isabel Luisa, Francisca López-Granados, Alfonso\nGarcía-Ferrer, José Manuel Peña-Barragán, Montserrat Jurado-Expósito,\nManuel Sánchez De La Orden, and María González-Audicana. 2009.\n“Object- and Pixel-Based Analysis for Mapping Crops and Their\nAgro-Environmental Associated Measures Using QuickBird Imagery.”\nComputers and Electronics in Agriculture 68 (2): 207–15. https://doi.org/10.1016/j.compag.2009.06.004.\n\n\nChoudhary, K., W. Shi, Y. Dong, and R. Paringer. 2022. “Random\nForest for Rice Yield Mapping and Prediction Using Sentinel-2 Data with\nGoogle Earth Engine.” Advances in Space Research 70 (8):\n2443–57. https://doi.org/10.1016/j.asr.2022.06.073.\n\n\nCohen, Sagy, Austin Raney, Dinuke Munasinghe, J. Derek Loftis, Andrew\nMolthan, Jordan Bell, Laura Rogers, et al. 2019. “The Floodwater\nDepth Estimation Tool (FwDET V2.0) for Improved Remote Sensing Analysis\nof Coastal Flooding.” Natural Hazards and Earth System\nSciences 19 (9): 2053–65. https://doi.org/10.5194/nhess-19-2053-2019.\n\n\nComber, Alexis, Peter Fisher, Chris Brunsdon, and Abdulhakim Khmag.\n2012. “Spatial analysis of remote sensing image classification\naccuracy遥感影像分类精度的空间分析.” Remote Sensing of\nEnvironment 127 (December): 237–46. https://doi.org/10.1016/j.rse.2012.09.005.\n\n\nDuro, Dennis C., Steven E. Franklin, and Monique G. Dubé. 2012. “A\nComparison of Pixel-Based and Object-Based Image Analysis with Selected\nMachine Learning Algorithms for the Classification of Agricultural\nLandscapes Using SPOT-5 HRG Imagery.” Remote Sensing of\nEnvironment 118 (March): 259–72. https://doi.org/10.1016/j.rse.2011.11.020.\n\n\nFakhri, Falah, and Ioannis Gkanatsios. 2021. “Integration of\nSentinel-1 and Sentinel-2 Data for Change Detection: A Case Study in a\nWar Conflict Area of Mosul City.” Remote Sensing\nApplications: Society and Environment 22 (April): 100505. https://doi.org/10.1016/j.rsase.2021.100505.\n\n\nFuleihan, Dean, Dominic Williams, Daniel A Zarrilli, and OneNYC\nDirector. n.d. “The City of New York Mayor Bill de Blasio.”\n\n\nHall-Beyer, Mryka. 2017a. “Practical Guidelines for Choosing GLCM\nTextures to Use in Landscape Classification Tasks over a Range of\nModerate Spatial Scales.” International Journal of Remote\nSensing 38 (5): 1312–38. https://doi.org/10.1080/01431161.2016.1278314.\n\n\n———. 2017b. “Practical Guidelines for Choosing GLCM Textures to\nUse in Landscape Classification Tasks over a Range of Moderate Spatial\nScales.” International Journal of Remote Sensing 38 (5):\n1312–38. https://doi.org/10.1080/01431161.2016.1278314.\n\n\n———. 2017c. “Practical Guidelines for Choosing GLCM Textures to\nUse in Landscape Classification Tasks over a Range of Moderate Spatial\nScales.” International Journal of Remote Sensing 38 (5):\n1312–38. https://doi.org/10.1080/01431161.2016.1278314.\n\n\nHirpa, Feyera A., Valerio Lorini, Simon J. Dadson, and Peter Salamon.\n2021. “Calibration of Global Flood Models.” In, 201–11.\nAmerican Geophysical Union (AGU). https://doi.org/10.1002/9781119427339.ch11.\n\n\nHunt, Merryn L., George Alan Blackburn, Luis Carrasco, John W. Redhead,\nand Clare S. Rowland. 2019. “High Resolution Wheat Yield Mapping\nUsing Sentinel-2.” Remote Sensing of Environment 233\n(November): 111410. https://doi.org/10.1016/j.rse.2019.111410.\n\n\n“Image Classification Techniques in Remote Sensing.” n.d.\nhttps://gisgeography.com/image-classification-techniques-remote-sensing/.\n\n\nKaplan, Gordana, Tatjana Rashid, Mateo Gasparovic, Andrea Pietrelli, and\nVincenzo Ferrara. 2022. “Monitoring War-Generated Environmental\nSecurity Using Remote Sensing: A Review.” Land Degradation\nand Development 33 (10): 1513–26. https://doi.org/10.1002/ldr.4249.\n\n\nKrówczyńska, Małgorzata, Edwin Raczko, Natalia Staniszewska, and Ewa\nWilk. 2020. “Asbestoscement Roofing Identification\nUsing Remote Sensing and Convolutional Neural Networks (CNNs).”\nRemote Sensing 12 (3): 408. https://doi.org/10.3390/rs12030408.\n\n\nLi, Xinchen, Liang Guo, and Jonathan Cheung-Wai Chan. 2025.\n“Combined Sentinel-1 and Sentinel-2 Imagery for Destroyed Building\nClassification in Gaza Strip with Random Forest.” IEEE\nJournal of Selected Topics in Applied Earth Observations and Remote\nSensing 18: 3827–39. https://doi.org/10.1109/JSTARS.2024.3522389.\n\n\nLu, Dengsheng, Mateus Batistella, Guiying Li, Emilio Moran, Scott\nHetrick, Corina Da Costa Freitas, Luciano Vieira Dutra, and Sidnei João\nSiqueira Sant’Anna. 2012. “Land Use/Cover Classification in the\nBrazilian Amazon Using Satellite Images.” Pesquisa\nAgropecuária Brasileira 47 (9): 1185–1208. https://doi.org/10.1590/S0100-204X2012000900004.\n\n\nMason, David C., Sarah L. Dance, and Hannah L. Cloke. 2023.\n“Toward Improved Urban Flood Detection Using Sentinel-1:\nDependence of the Ratio of Post- to Preflood Double Scattering Cross\nSections on Building Orientation.” Journal of Applied Remote\nSensing 17 (1): 16507. https://doi.org/10.1117/1.JRS.17.016507.\n\n\nMueller, Hannes, Andre Groeger, Jonathan Hersh, Andrea Matranga, and\nJoan Serrat. 2021. “Monitoring War Destruction from Space Using\nMachine Learning.” Proceedings of the National Academy of\nSciences of the United States of America 118 (23): 1–9. https://www.jstor.org/stable/27040853.\n\n\n“NYS Flood Impact Decision Support System - FIDSS.” n.d. https://sedac.ciesin.columbia.edu/mapping/nysfidss/?page=NYS-FIDSS&views=About.\n\n\nSharma, Ram, Keitarou Hara, and Ryutaro Tateishi. 2017.\n“High-Resolution Vegetation Mapping in Japan by Combining\nSentinel-2 and Landsat 8 Based Multi-Temporal Datasets Through Machine\nLearning and Cross-Validation Approach.” Land 6 (3): 50.\nhttps://doi.org/10.3390/land6030050.\n\n\nTabib Mahmoudi, Fatemeh, Alireza Arabsaeedi, and Seyed Kazem Alavipanah.\n2019. “Feature-Level Fusion of Landsat 8 Data and SAR Texture\nImages for Urban Land Cover Classification.” Journal of the\nIndian Society of Remote Sensing 47 (3): 479–85. https://doi.org/10.1007/s12524-018-0914-8.\n\n\n“THE 17 GOALS | Sustainable Development.” n.d. https://sdgs.un.org/goals.\n\n\nTommasini, Maurizio, Alessandro Bacciottini, and Monica Gherardelli.\n2019a. “A QGIS Tool for Automatically Identifying Asbestos\nRoofing.” ISPRS International Journal of Geo-Information\n8 (3). https://doi.org/10.3390/ijgi8030131.\n\n\n———. 2019b. “A QGIS Tool for Automatically Identifying Asbestos\nRoofing.” ISPRS International Journal of Geo-Information\n8 (3). https://doi.org/10.3390/ijgi8030131.\n\n\nXue, D., X. Yu, S. Jia, F. Chen, and X. Li. 2018. “Study on\nLandslide Disaster Extraction Method Based on Spaceborne Sar Remote\nSensing Images  Take Alos Palsar for an Example.”\nThe International Archives of the Photogrammetry, Remote Sensing and\nSpatial Information Sciences XLII-3 (April): 2023–27. https://doi.org/10.5194/isprs-archives-XLII-3-2023-2018.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CASA0023 Learning Diary",
    "section": "",
    "text": "Personal Introduction\nMy name is Borui. I am from Beijing. I did my undergraduate degree at China Agricultural University and majored in Landscape Architecture. During that time, I became interested in landscape design, which covered topics such as food security, climate change, urban segregation, etc. This is my portfolio from undergrad. I am also interested in the challenges that cities will face due to climate change, such as flooding, and chose a related topic for my undergraduate final project.\nNow, I am studying MSc in spatial data science. I hope to be able to analyse the problems in urban space using data sciences and to improve my quantitative analysis and coding skills.\nI took this module Remotely Sensing Cities and Environments to learn more about environmental hazards arising from a changing climate. Through learning and operationalising remotely sensed Earth observation data, I hope I can provide effective recommendations for today’s cities or urban planning.\nThis diary will record my learning process throughout the module. It includes a summary of knowledge from each lesson, practical application and personal reflective thinking. I hope to be able to show what I have learnt from this module in this way.\nCertain parts of this diary have been improved with the help of ChatGPT, including simpler table descriptions, grammar checks, and the reduction of repetitive sentences.",
    "crumbs": [
      "Personal Introduction"
    ]
  },
  {
    "objectID": "Wk2_Presentation.html",
    "href": "Wk2_Presentation.html",
    "title": "2  Xaringan Presentation",
    "section": "",
    "text": "This week we tried to use markdown tools like Xaringan and Quarto to show our idea on the website with code. How to adjust the templates and hide the code blocks was difficult for me during the learning process. Creating presentations through code gives the presentation file greater presentational reproducibility and flexibility. Although the process of learning is difficult, it can give me a strong sense of achievement. Here is a presentation that introduces the functions of the Sentinel-2, created through the Xaringan.\n\n\n\n\n\n\n\n\nView Full PowerPoint",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Xaringan Presentation</span>"
    ]
  },
  {
    "objectID": "Wk3_Corrections.html#summary",
    "href": "Wk3_Corrections.html#summary",
    "title": "3  Corrections and Enhancements",
    "section": "",
    "text": "3.1.1 Correction\nThe correction makes the raw remote-sensing data become “Analysis Ready Data”(ARD). In this lecture, we mainly learn four kinds of correction: Geometric Correction, Atmospheric Correction, Orthorectification, and Radiometric Correction. I try to figure out the connection between them to have a better understanding. It seems like the Radiometric correction and atmospheric correction are the most common correction steps in remote sensing data processing, and are usually required for every image. Geometric Correction and Orthorectification are commonly used processing scenarios where spatial aberrations exist or where accurate spatial analysis is required, not every data is needed. The following table will introduce details about different corrections. Regression is important in correction methods.\n\n\n\nCorrection methods\n\n\nCorrection\npurposes /defination\nmethods\ninput data\noutput\nApplications\n\n\n\n\nGeometric Correction\nCorrects distortions caused by the sensor or satellites to align the image with the geographic coordinate system, ensuring spatial accuracy.\ndifferent transformation types(linear, polynomial, Helmert…)\nPixel coordinates of the original image, GCPs（Ground Control Points, Pixel coordinates of the rectified (gold standard) map\ncorrected image\nThe basis for subsequent georectification and orthorectification\n\n\nAtmospheric Correction\nRemoval of environmental attenuation:Atmospheric scattering and Topographic attenuation\nDOS（Dark object subtraction, PIFs (Psuedo-invariant Features), Py6S, FLAASH, ACORN, QUAC, ATCOR Empirical Line Correction\nradiance reflected above the surface , Atmospheric attenuation\nTop-of-atmosphere reflectance and Surface reflectance\nBiophysical parameters required and the use of spectral signatures across time and space.\n\n\nOrthorectification\nremoving distortions. making the pixels viewed at the nadir (straight down)\nCosine correction, Minnaert correction, Statistical Empirical correction, C Correction (advancing the Cosine)\nradiance, Sun's zenith angle, Sun's incidence angle slope angle (from DEM), slope aspect, solar zenith, solar azimuth\ngeographically aligned and topography removed\nOrthorectification Is a subset of georectification, useful for Precise Georeferencing and High-Precision Measurement and Spatial Analysis\n\n\nRadiometric Calibration\nConverts the digital value (DN) of an image into a physical sense of radiant brightness or reflectance, eliminating the effects of sensors, light sources, etc.\nLλ = Bias + (Gain * DN)\nDN, sensor information, Solar radiation information\nCorrected image\nOften used as a first step in data pre-processing to provide a basis for subsequent atmospheric corrections, etc.\n\n\n\n\n\n\n\n\n\n3.1.2 Joining the data\nIn some cases, we need to merge the images from different regions and times to cover the study area. We had better choose the image from the same day or else we need to standardise data. In remote sensing, it is called “Mosaicking”. This involves complex arithmetic processes: histogram matching algorithm feathering and blending. But it is simple in R just needs some time.\n\nm1 &lt;- terra::mosaic(l83, l84, fun=\"mean\")\n\n\n\n3.1.3 Image Enhancement\nWe choose the appropriate data, process it, and modify it to generate the desired output. The methods help us change the image and produce the output for our purpose. The methods are as follows.\n\n\n\nImage enhancement methods\n\n\nEnhancement\npurposes /defination\nmethods\ninput data\nAdvantages\nApplicable scenarios\n\n\n\n\nContrast Enhancement\nImproved visualisation of the image by widening the distribution of pixel values.\nMinimum- Maximum Percentage Linear and Standard Deviation Piecewise Linear Contrast Stretch\nImages with a narrow distributions\nEnhance images contrast and details\npre-processing for subsequent classification and visual interpretation\n\n\nBand Ratioing\nUsing the ratio of reflectance of different bands to highlight specific feature characteristics\ndivision operations (like NDVI, NDBI, tasselled cap etc.)\nMulti-spectral imagery\nEmphasis on relative differences in expression of feature characteristics\nNormalized Burn Ratio, The Normalised Difference Vegetation Index, the Normalized Difference Moisture Index (NDMI)\n\n\nFiltering\nSuppressing noise or highlighting local variations through filter operations.\nLow pass or low frequency (averages the surrounding pixels)/High pass or high frequency-enhance local variations\nOriginal data\nSmooth or sharpen images; extract edge information as required\nFeature extraction, target detection\n\n\nPCA\nDimensionality reduction, removal of inter-band correlations, and extraction of most of the information in the image.\nIn R this is prcomp() from the terra package\nMulti-spectral imagery\nExtracting the main direction of change reduces the computational burden and noise reduction\nData compression, change detection, feature extraction\n\n\nTexture\nmeasuring the relationships between pixel-to-pixel, quantifying roughness to obtain feature characteristics.\nGLCM(Gray Level Co-occurrence Matrix ) second-order texture measures, third-order texture measures\nGrey-scale images or single-band data, windows\nCharacterisation of feature texture, feature classification, target identification\nLand cover classification, target detection, environmental monitoring\n\n\nFusion\nFusion of data from different sensors or different times\nPan-sharpening, Gram-Schmidt, IHS\nMulti-temporal, multi-sensor data\nenhance the details of the images\nHigh resolution data generate, changes detection, data integration",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Corrections and Enhancements</span>"
    ]
  },
  {
    "objectID": "Wk3_Corrections.html#application",
    "href": "Wk3_Corrections.html#application",
    "title": "3  Corrections and Enhancements",
    "section": "3.2 Application",
    "text": "3.2 Application\nTexture analysis is widely used as an image enhancement technique to improve classification accuracy by providing additional spatial information that spectral bands alone cannot capture.\nLu et al. (2012) improved the identification of the Amazonian tropics by using texture fusion with multi-spectral images. By incorporating GLCM texture measures, classification accuracy improved in distinguishing dense forests from secondary vegetation. This demonstrates how texture-based enhancement refines image details and supports better feature extraction. However, some texture measure has high correlations with each other, leading to increased redundant information and possibly affecting classification stability. Although the author finally chose two measures, sifting through all the variables still requires a lot of computation.\nWhich texture-measures combination from GLCM is most suitable for landscape identification? Hall-Beyer (2017a) examines scenarios such as agricultural landscapes. Trying different texture measures in the identification of natural landscapes. Combining texture with the PCA method can filter the most representative texture features and improve computational efficiency. Certain texture features, such as Contrast and Correlation, are particularly effective in capturing spatial variations, while others, like Entropy, provide additional complexity. Different landscapes require different texture measures, as structural complexity varies across environments.\n\n\n\nTexture measures Source: Remote Sensing for Dummies. Source:Texture and PCA in R\n\n\nHall-Beyer (2017a) PCA to help with filtering texture measures. Reduced workload in identifying natural landscapes for future research. However, this research only discusses the different groups of texture measures that will bring different influences to the results. It does not mention the influences of resolution and window sizes. Smaller windows are more effective for detecting fine-scale features, while larger windows enhance broader landscape classification, making window selection an important factor in image enhancement. So, in the future, research can focus on the different window sizes for different-resolution images. This will improve the applicability of the texture method at different spatial scales. Now, texture is usually used for natural landscapes like forests. But Ent can detect unnatural edges well. So texture is also important for urban land use identification. For example, Fatemeh Tabib Mahmoudi, Arabsaeedi, and Alavipanah (2019) introduced entropy, contrast, correlation and mean for urban object recognition. In the future there could be more research on applying texture in complex scenarios like cities. Future research should explore integrating texture-based enhancement with other image correction techniques, such as radiometric correction, to improve classification robustness across varying imaging conditions.\n\n\n\nDifferent texture measures visual appearance. Source:(Hall-Beyer 2017b)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Corrections and Enhancements</span>"
    ]
  },
  {
    "objectID": "Wk3_Corrections.html#reflection",
    "href": "Wk3_Corrections.html#reflection",
    "title": "3  Corrections and Enhancements",
    "section": "3.3 Reflection",
    "text": "3.3 Reflection\nThis week we learned about Correction, Mosaicking, and image enhancement. We usually comprehend different methods and concepts separately during the learning process. (But the content this week is still hard for me.) In practice, for different bands and spatial resolutions, the effects of the different methods are also different, so it is necessary to combine various methods to realise the purpose of the study. For example, when doing principal component analysis (PCA) on spectral data, the subsequent eigenchannels usually represent indexes like NDVI (vegetation) mentioned in the band ratio.(Hall-Beyer 2017c) This suggests that there may be a relationship of complementarity between the different approaches. In addition, in order to get more clearer images and results, researchers often combine different sensor data and methods of analysis.\nThe enhancement method—Texture, currently applies to natural landscape identification such as forests. But I think it can also used in small-scale urban identification because Ent is sensitive to unnatural edges and can be used to identify artificial landscapes. What’s more, compared with popular Spectroscopic methods, Texture methods (e.g. GLCM) focus on the spatial pattern of the image and may reduce the band radiometric errors due to temperature variation caused by the city heat island effect. However, texture can only be used for enlarged objects due to its large computation and the edges of the city are especially complex, which limits its ability to complement other methods in many situations. I believe addressing this limitation could be an important direction for future research, particularly through deep learning-based feature extraction. Automated texture analysis could reduce computational demands and improve classification efficiency, making it more applicable to urban studies. In specific applications, texture features might be beneficial for urban planning, by accurately identifying traffic-congested road sections based on density variations.\n\n\n\n\nHall-Beyer, Mryka. 2017c. “Practical Guidelines for Choosing GLCM Textures to Use in Landscape Classification Tasks over a Range of Moderate Spatial Scales.” International Journal of Remote Sensing 38 (5): 1312–38. https://doi.org/10.1080/01431161.2016.1278314.\n\n\n———. 2017a. “Practical Guidelines for Choosing GLCM Textures to Use in Landscape Classification Tasks over a Range of Moderate Spatial Scales.” International Journal of Remote Sensing 38 (5): 1312–38. https://doi.org/10.1080/01431161.2016.1278314.\n\n\n———. 2017b. “Practical Guidelines for Choosing GLCM Textures to Use in Landscape Classification Tasks over a Range of Moderate Spatial Scales.” International Journal of Remote Sensing 38 (5): 1312–38. https://doi.org/10.1080/01431161.2016.1278314.\n\n\nLu, Dengsheng, Mateus Batistella, Guiying Li, Emilio Moran, Scott Hetrick, Corina Da Costa Freitas, Luciano Vieira Dutra, and Sidnei João Siqueira Sant’Anna. 2012. “Land Use/Cover Classification in the Brazilian Amazon Using Satellite Images.” Pesquisa Agropecuária Brasileira 47 (9): 1185–1208. https://doi.org/10.1590/S0100-204X2012000900004.\n\n\nTabib Mahmoudi, Fatemeh, Alireza Arabsaeedi, and Seyed Kazem Alavipanah. 2019. “Feature-Level Fusion of Landsat 8 Data and SAR Texture Images for Urban Land Cover Classification.” Journal of the Indian Society of Remote Sensing 47 (3): 479–85. https://doi.org/10.1007/s12524-018-0914-8.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Corrections and Enhancements</span>"
    ]
  },
  {
    "objectID": "Wk4_Policy.html#international",
    "href": "Wk4_Policy.html#international",
    "title": "Policy",
    "section": "1.1.1 International",
    "text": "1.1.1 International\nThe 2030 Agenda for Sustainable Development also mentioned climate risk reduction in its reports. It is an important part of the 17 Sustainable Development Goals (SDGs). These SDGs build on decades of work by countries and the UN.(“THE 17 GOALS | Sustainable Development,” n.d.)\nSDG11: Make cities and human settlements inclusive, safe, resilient and sustainable\nSDG9: Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation.\nOther historical documents and meetings also mention the challenges caused by flooding, such as the Sendai Framework for Disaster Risk Reduction, which provides guidance for disaster prevention and control, the Guidelines for Reducing Flood Losses launched, and the Hyogo Framework for Action (2005-2015).",
    "crumbs": [
      "Policy"
    ]
  },
  {
    "objectID": "Wk4_Policy.html#metropolitan",
    "href": "Wk4_Policy.html#metropolitan",
    "title": "Policy",
    "section": "1.1.2 Metropolitan",
    "text": "1.1.2 Metropolitan\nA LIVABLE CLIMATE is also a goal In OneNYC 2050, it proposed 4 relative initiatives for building sustainable cities. It advocates strengthening communities, buildings, infrastructure, and the waterfront to be more resilient. (21) To make the city more resilient and safer. The government also collaborated with the U.S. ARMY CORPS OF ENGINEERS to reshape the shoreline of New York. The main measures are divided into three directions: Protecting the neighbourhoud and basic infrastructure, effective management measures and more accurate research and forecasting.\n\n\n\nFlood map(Source:OneNYC 2050)",
    "crumbs": [
      "Policy"
    ]
  },
  {
    "objectID": "Wk4_Policy.html#local",
    "href": "Wk4_Policy.html#local",
    "title": "Policy",
    "section": "1.1.3 Local",
    "text": "1.1.3 Local\nAt the local level, the city will build according to the adopting flood maps which delineate the climate projections. For example, in Lower Manhattan, the city extends the shoreline to the East River, which is 20 meters above the current sea level and set up many seafront parks.\n\n\n\nLocal policy: Lower Manhattan (Source:OneNYC 2050)\n\n\nIn conclusion, current policies are already comprehensive and specific, to explore the effectiveness of these initiatives, I think needs to be made through a data approach.",
    "crumbs": [
      "Policy"
    ]
  },
  {
    "objectID": "Wk4_Policy.html#application",
    "href": "Wk4_Policy.html#application",
    "title": "4  Policy",
    "section": "4.2 Application",
    "text": "4.2 Application\n\n4.2.1 Data and Model\nUsually, flood hazard mapping always includes two parts: the flood models (FL) and natural catastrophe (CAT) models. (Hirpa et al. 2021) Flood models are usually based on DEMs, channel locations, stream flow, and precipitation data. The catastrophe models always include land cover data, critical infrastructure, and building footprints extracted from LIDAR data.New York has already drawn a series of maps to inform flood planning and Response. For example, the NYC Stormwater Flood Maps (“NYS Flood Impact Decision Support System - FIDSS,” n.d.) shows the area’s potential flooding scenarios. New York City makes flooding monitoring with the help of FloodNet Sensors.\n\n\n\nNYC Stormwater Flood Maps (Source: New York City Stormwater Flood Maps)\n\n\nAlthough the hydrological modelling in the local area seems to be progressively improving perfectly. Remote sensing data is still helpful in emergency response and large-scale analyses of flooding. We can get the depth and extent of the flood by Landsat series data. By computing the modified normalized difference water index (NDWI) to do water detection after floodings. (Cohen et al. 2019) We can get flooded areas by comparing it with the Land Cover Dataset. However, Landsat data may be influenced by weather so can not get continuous data. SAR sensors like Sentinel-1 have a higher resolution (10m) and can detect through clouds, so it is more commonly applied to real-time flood monitoring. The flooding analyses estimate the post-to-preflood radar cross-section (RCS) ratio and exclude the double scatterers (DSs) in urban areas.(Mason, Dance, and Cloke 2023) We can also get the depth of the flood by analysing it in relation to the high-resolution DEM data obtained from LIDAR. In flood detection we can fuse sar and optical satellite imagery in pursuit of more accurate predictions.\n\n\n\nLandsat flooding analysis. Source:Cohen et al. (2019)\n\n\nThe Land Cover Dataset is important in The Social Vulnerability analyses. We can obtain large area land cover monitoring through optical remote sensing like Landsat, Sentinel-2, and MODIS. Using different indexes helps us detect different types of covering.\nNDVI: distinguishes vegetation from non-vegetation.  \n\nNDBI: extracting building area.  \n\nNDWI: extracting body of water.  \nThe DEM Dataset is very important in remote sensing and hydrological analysis. In the local area, we often use LIDAR to get 1-5m DEM data for more precise analyses. For larger study areas like cities, we can use SAR to get DEM data to simplify the dataset for ease of computation.\n\n\n4.2.2 Workflows\n\n\n\nWorkflow image",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Policy</span>"
    ]
  },
  {
    "objectID": "Wk4_Policy.html#data-and-model",
    "href": "Wk4_Policy.html#data-and-model",
    "title": "Policy",
    "section": "1.2.1 Data and Model",
    "text": "1.2.1 Data and Model\nUsually, flood hazard mapping always includes two parts: the flood models (FL) and natural catastrophe (CAT) models. () The flood models are usually based on the hydrological modelling with DEM, channel location, river discharge and Precipitation data. The catastrophe models always include Land cover data and critical infrastructure and building footprints extracted from LIDAR data. To support the flood information decision, New York has already drawn a map to inform flood planning and Response. For example, the NYC Stormwater Flood Maps () shows the area’s potential flooding scenarios. New York City now makes flooding monitoring with the help of FloodNet Sensors Which are 87 Low-Cost Ultrasonic sensors, which bring more precision results.\n\n\n\nNYC Stormwater Flood Maps (Source: New York City Stormwater Flood Maps)\n\n\nAlthough the hydrological modelling in the local area seems to be progressively improving perfectly. Remote sensing data is still helpful in emergency response and large-scale analyses of flooding. We can get the depth and extent of the flood by Landsat series data. By computing the modified normalized difference water index (NDWI) to do water detection after floodings. (Cohen et al. 2019) We can get flooded areas by comparing it with the Land Cover Dataset. However, Landsat data may be influenced by weather so can not get continuous data. SAR sensors like Sentinel-1 have a higher resolution (10m) and can detect through clouds, so it is more commonly applied to real-time flood monitoring. The flooding analyses estimate the post-to-preflood radar cross-section (RCS) ratio and exclusion of double scatterers (DSs) in urban areas.(Mason, Dance, and Cloke 2023) We can also get the depth of the flood by analysing it in relation to the high-resolution DEM data obtained from LIDAR.\n\n\n\nLandsat flooding analysis\n\n\nThe Land Cover Dataset is important in The Social Vulnerability analyses. We can obtain large area land cover monitoring through optical remote sensing like Landsat, Sentinel-2, and MODIS. Using different indexes helps us detect different types of covering.\nNDVI: distinguishes vegetation from non-vegetation.  \n\nNDBI: extracting building area.  \n\nNDWI: extracting body of water.  \nThe DEM Dataset is very important in flooding analyses. It’s very important in remote sensing and hydrological analysis. In the local area, we often use LIDAR to get 1-5m DEM data for more precise analyses. For larger study areas like cities, we can use SAR to get DEM data to simplify the dataset for ease of computation.",
    "crumbs": [
      "Policy"
    ]
  },
  {
    "objectID": "Wk4_Policy.html#reflection",
    "href": "Wk4_Policy.html#reflection",
    "title": "4  Policy",
    "section": "4.3 Reflection",
    "text": "4.3 Reflection\nCurrent NYC Flood Maps may be plots based on hydrologic models developed through hydrologic analyses. New York is a coastal city with many important rivers, the analysis will be difficult because of the different hydrological properties of sea and river water. I think using remote sensing data for analysis is simpler. Because water has the same reflective properties, and the analysis should focuse on the results rather than the complex hydrological process. It is more suitable for large-scale analysis across the entire city like New York. Moreover, remote sensing enables long-term monitoring of flood impacts, providing valuable data for evaluating policy effectiveness before and after implementation. For example, New York takes many projects to protect Lower Manhattan and plots the project region and location on the map, reflecting zoning planning, which is very specific and highly instructive.Remote sensing data can provide clear changes before and after the implementation of the policy, and provide support for those projects. A Data-Driven Approach to urban planning analyses data from different levels with different methods leading to more efficient and accurate policy delivery.\nThe methods applied in New York could be adapted for other coastal cities，like Tokyo, Shanghai, which face similar climate challenges. Unlike with radar detectors or precise hydrological data, open-access satellite data (Sentinel-1, Landsat) offers a cost-effective strategy for flood risk analysis and feedback on policy effectiveness, particularly in developing country with limited resources.\n\n\n\n\nCohen, Sagy, Austin Raney, Dinuke Munasinghe, J. Derek Loftis, Andrew Molthan, Jordan Bell, Laura Rogers, et al. 2019. “The Floodwater Depth Estimation Tool (FwDET V2.0) for Improved Remote Sensing Analysis of Coastal Flooding.” Natural Hazards and Earth System Sciences 19 (9): 2053–65. https://doi.org/10.5194/nhess-19-2053-2019.\n\n\nHirpa, Feyera A., Valerio Lorini, Simon J. Dadson, and Peter Salamon. 2021. “Calibration of Global Flood Models.” In, 201–11. American Geophysical Union (AGU). https://doi.org/10.1002/9781119427339.ch11.\n\n\nMason, David C., Sarah L. Dance, and Hannah L. Cloke. 2023. “Toward Improved Urban Flood Detection Using Sentinel-1: Dependence of the Ratio of Post- to Preflood Double Scattering Cross Sections on Building Orientation.” Journal of Applied Remote Sensing 17 (1): 16507. https://doi.org/10.1117/1.JRS.17.016507.\n\n\n“NYS Flood Impact Decision Support System - FIDSS.” n.d. https://sedac.ciesin.columbia.edu/mapping/nysfidss/?page=NYS-FIDSS&views=About.\n\n\n“THE 17 GOALS | Sustainable Development.” n.d. https://sdgs.un.org/goals.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Policy</span>"
    ]
  },
  {
    "objectID": "Wk4_Policy.html#workflows",
    "href": "Wk4_Policy.html#workflows",
    "title": "Policy",
    "section": "1.2.2 Workflows",
    "text": "1.2.2 Workflows\nThe New York government now retrofitted our nearly 1 million buildings and invested more than $20 billion against extreme weather events.\n\n\n\nWorkflow image",
    "crumbs": [
      "Policy"
    ]
  },
  {
    "objectID": "Wk4_Policy.html#summary",
    "href": "Wk4_Policy.html#summary",
    "title": "4  Policy",
    "section": "",
    "text": "4.1.1 International\nThe 2030 Agenda for Sustainable Development also mentioned climate risk reduction in its reports.(“THE 17 GOALS | Sustainable Development,” n.d.)\nSDG11: Make cities and human settlements inclusive, safe, resilient and sustainable\nSDG9: Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation.\nOther historical documents and meetings also mention the challenges caused by flooding, such as the Sendai Framework for Disaster Risk Reduction, which provides guidance for disaster prevention and control, the Guidelines for Reducing Flood Losses launched, and the Hyogo Framework for Action (2005-2015).\n\n\n4.1.2 Metropolitan\nLivable climate is also set as a key goal in OneNYC 2050, it proposed 4 relative initiatives for building sustainable cities. It also advocates strengthening communities, buildings, infrastructure, and the waterfront to be more resilient. (21) To make the city more resilient and safer. The government also collaborated with the U.S. ARMY CORPS OF ENGINEERS to reshape the shoreline of New York.\n\n\n\nFlood map(Source:OneNYC 2050)\n\n\n\n\n4.1.3 Local\nAt the local level, the city will build according to the adopting flood maps which delineate the climate projections. For example the East Side Coastal Resiliency (ESCR) Project, in Lower Manhattan, the city extends the shoreline to the East River, which will be 20 meters above the current sea level and set up many seafront parks.\n\n\n\nLocal policy: Lower Manhattan (Source:OneNYC 2050)\n\n\nIn conclusion, current policies are already comprehensive and specific. I think the effectiveness of these initiatives needs to be explored through data methods.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Policy</span>"
    ]
  },
  {
    "objectID": "Wk6_GEE.html",
    "href": "Wk6_GEE.html",
    "title": "5  An introduction to Google Earth Engine",
    "section": "",
    "text": "5.1 Summary\nGoogle Earth Engine(GEE) can access and tackle Geographic information data quickly by using a cloud computing platform. I think GEE has two main benefits.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>An introduction to Google Earth Engine</span>"
    ]
  },
  {
    "objectID": "Wk6_GEE.html#summary",
    "href": "Wk6_GEE.html#summary",
    "title": "5  An introduction to Google Earth Engine",
    "section": "",
    "text": "5.1.1 Benefits\n\nGEE is lazy computing for users compared with traditional data analysis methods like ArcGIS, R and Python. GEE data preprocessing operations, such as reprojection and uniform resolution, can be handled automatically. So, the data in GEE is analysis-ready. It helps users to focus on data analysis and application work.\nGEE can access and deal with large-scale data, especially global remote sensing data. As a cloud-based platform, it is easy for researchers to disseminate their results to others, making the project more reproducible. No need to upload local files to GitHub (which has a size limit, and most remote sensing images can’t be uploaded) to share the results as before.\n\n\n\n\n\n\n\nGithub limitation(chatgpt generated)\n\n\n\n\nLarge-scale dataset storage loading process uses a pyramid of downsampled tiles, reducing the image to multiple scales until it reaches 256 × 256 pixels per tile. Making the processing more efficient. Here, I use the image and the example of LandSat with 30m resolution to help me understand it.\n\n\n\nResolution Pyramid Source: GEE\n\n\nIn GEE, we can perform lots of processes, like reducing images, linear regression, image enhancements, etc. GEE is mainly programmed with JavaScript; compared with R, the data tackling is more simplified. The table shows the differences between GEE and R.\n\n\n\nThe differences between GEE and R\n\n\nFeature\nR (Local Computing)\nGEE (Cloud Computing)\n\n\n\n\nData Storage\nData is saved on your computer's memory or hard drive.\nData is stored on Google's servers, and your code just refers to it.\n\n\nComputation\nRuns immediately on your computer (eager evaluation).\nRuns only when needed (lazy computation).\n\n\nLooping\nYou can use for loops and apply() to process data.\nYou can't use for loops on ee objects. Instead, you need to use map().\n\n\nVisualization\nNeeds ggplot2 or tmap to draw maps.\nYou can show maps directly with Map.addLayer() on an interactive map.\n\n\nImage data\na stack of rasters\nImage collection\n\n\nData Access\nYou must download data (e.g., Landsat, Sentinel) before using it.\nYou can access data directly from Google Earth Engine’s Data Catalog (no downloading needed).\n\n\nProjection & Resolution\nYou must manually adjust projection and resolution (st_transform(), projectRaster()).\nGEE automatically handles projection and resampling.\n\n\nMachine Learning\nUses external ML libraries like randomForest and xgboost.\nHas built-in ML tools (like Random Forest), but can't use external ML libraries.\n\n\nLimits\nOnly limited by your computer’s power, no API limits.\nFree accounts have limits on computing and storage. Pro accounts are needed for more resources.\n\n\n\n\n\n\n\n\n\n5.1.2 Limitations\nGEE is used in a variety of study areas like global forest change, crop yield estimation, fire recovery, etc. because of its efficient access and computation and the wealth of data types and functions in the library. It can especially show its benefits when it comes to large-scale data or time series analyses. However, it also has limitations. 1. It is influenced by Google’s server resource allocation; Google’s server resources affect request limits, including how long they run and how many can run at the same time.. That may lead to the failure of some complicated computation. 2. It may not work well for operations involving value can be influenced by arbitrarily distant input or data not covered by the GEE library.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>An introduction to Google Earth Engine</span>"
    ]
  },
  {
    "objectID": "Wk6_GEE.html#application",
    "href": "Wk6_GEE.html#application",
    "title": "5  An introduction to Google Earth Engine",
    "section": "5.2 Application",
    "text": "5.2 Application\nIn week one, I have explored what influence the data from different sensors would have on the analysis of crop yield estimation. This week, I want to focus on the difference of analyses of GEE. I conclude two articles to make comparisons, both of them using a vegetation index to set up a random forest model to predict the crop yield. But one of them used GEE to analyse, the other used traditional methods.\n\nThe role of GEE in data processing\n\nChoudhary et al. (2022) used GEE to remove all errors and get similar weight to tackle the data quickly. These steps help quickly access multi-temporal Sentinel-2 data and reduce the time to deal with data. But GEE data tackling may ignore the unusual weather or special local terrain, which may reduce the prediction accuracy. Hunt et al. Hunt et al. (2019) did data pre-processing manually (like using a buffer to remove gaps from data cleaning), selecting a date image with fewer clouds and using a Sen2Cor processor to correct data, improving the quality of input data, so the model performances are better.\n\nPerformance of random forests in GEE and non-GEE environments\n\nHunt et al. Hunt et al. (2019) using different combinations of data( meteorological, terrain, and soil moisture content) to improve the RF model prediction result in wheat production. Precision of yield estimation at field level (RMSE 0.61 t/ha) is a specific estimation method that can be modified with manual feature adjustment to select model parameters and data input. This approach is more suitable for small-scale, high-precision farm management. Choudhary et al. Choudhary et al. (2022) used GEE for efficient Calculation of rice yield and can quickly complete comparative testing of multiple models (RF, linear regression, decision trees). However, the model accuracy is (RMSE 0.40 t/ha) lower than traditional methods. The advantage of GEE is that it is suitable for large-scale agricultural monitoring. It is easy to analyse vegetation dynamics across seasons and over long periods of time using GEE. GEE can automate data handling processes, although it limits variable screening and manual model simulations affect accuracy, but it is suitable for large scale learning.\n\n\n\n\n\nLocal Computing Source: Hunt et al. (2019)\n\n\n\n\n\n\n\nGEE Cloud Platform Source: Choudhary et al. (2022)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>An introduction to Google Earth Engine</span>"
    ]
  },
  {
    "objectID": "Wk6_GEE.html#reflection",
    "href": "Wk6_GEE.html#reflection",
    "title": "5  An introduction to Google Earth Engine",
    "section": "5.3 Reflection",
    "text": "5.3 Reflection\nAfter comparing the above two articles, according to the features of the plant, I think it is more appropriate to exchange the tools used by the article. Wheat is mostly grown in the plains with larger areas per farmland, which has been put into mechanised production on a large scale. So, it is more suitable for larger-scale predictions using GEE. GEE can be monitored across multiple growing seasons, providing data for large-scale crop yield estimation, supply chain regulation and reduction monitoring. Field crops such as corn and soybeans are also well-suited for GEE analysis.\nLocal calculation can use high resolution commercial remotely sensed data (like quick bird) is more suitable for rice. Because rice is a crop with small field sizes and complex growing practices (e.g. terraced planting, mixed cropping) that requires careful management. Specific local calculation can combine local climate and terrain data to classify and analyse more accurately which support the intensive farming needs such as pest and disease monitoring, soil and water conservation, etc. GEE is limited by its resolution, making it difficult to meet the accuracy needs However, high-resolution remote sensing data is expensive, now is only available using in Japan and Korea. For other rice-growing Asian countries it is hard to promote. So, GEE will help these country access to remote-sensing data with low-cost, extensive crop monitoring and food security in those place. In future, the process depends not only on data quality and computational power but also can combine with crop growth characteristics, agricultural production patterns and economic viability.\n\n\n\n\nChoudhary, K., W. Shi, Y. Dong, and R. Paringer. 2022. “Random Forest for Rice Yield Mapping and Prediction Using Sentinel-2 Data with Google Earth Engine.” Advances in Space Research 70 (8): 2443–57. https://doi.org/10.1016/j.asr.2022.06.073.\n\n\nHunt, Merryn L., George Alan Blackburn, Luis Carrasco, John W. Redhead, and Clare S. Rowland. 2019. “High Resolution Wheat Yield Mapping Using Sentinel-2.” Remote Sensing of Environment 233 (November): 111410. https://doi.org/10.1016/j.rse.2019.111410.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>An introduction to Google Earth Engine</span>"
    ]
  },
  {
    "objectID": "Wk7_Clas1.html",
    "href": "Wk7_Clas1.html",
    "title": "6  Classification Ⅰ",
    "section": "",
    "text": "6.1 Summary\nThis week’s learning covered image classification. This is based on the pattern recognition or machine learning to turn every pixel in the image into one of classification. Image classification is applied in many areas like agriculture monitoring, air pollution, forest monitoring and urban green spaces assessment.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification Ⅰ</span>"
    ]
  },
  {
    "objectID": "Wk7_Clas1.html#conclusion-of-the-process-of-analysing-remotely-sensed-images",
    "href": "Wk7_Clas1.html#conclusion-of-the-process-of-analysing-remotely-sensed-images",
    "title": "6  Classification Ⅰ",
    "section": "6.2 Conclusion of the process of analysing remotely sensed images",
    "text": "6.2 Conclusion of the process of analysing remotely sensed images\nCombining the previous studies and articles, I think the analysis of remotely sensed images includes: 1. Data obtained – 2 pre-processed- 3. image enhancement-4. Factor choosing -5. Predict modelling(classification/regression)-6. Result assessment. Image classification belongs to the fifth step, which directly affects the final research results.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification Ⅰ</span>"
    ]
  },
  {
    "objectID": "Wk7_Clas1.html#approaches-to-image-classification",
    "href": "Wk7_Clas1.html#approaches-to-image-classification",
    "title": "6  Classification Ⅰ",
    "section": "6.3 Approaches to image classification",
    "text": "6.3 Approaches to image classification\n\n6.3.1 Unsupervised classification\nThese methods are applied to identify data with unknown land cover classes. Data were labelled into different categories using ML methods like clustering (k-means, DB-scan) based on spectral features. This reduces the influence of the human factor. The initial result of unsupervised classification may have mixed categories, so it always needs downscaling methods like PCA to optimise.\n\n\n\nUnsupervised classification Source: [GISgeography](https://gisgeography.com/image-classification-techniques-remote-sensing/)\n\n\n\n\n6.3.2 Supervised classification\nThese methods are the Classifier learns pattern in the data and puts the labels onto new data. It includes Parametric methods and Non-parametric methods. Recent studies tend to use machine learning/expert systems or spectral hybrid analyses. The steps in supervised classification include class definition, pre-processing, training, pixel assignment, and accuracy assessment.\n\n\n\nSupervised classification Source: [GISgeography](https://gisgeography.com/image-classification-techniques-remote-sensing/)\n\n\n\n6.3.2.1 Parametric methods\nThey are based on the assumption that the data follow a certain distribution pattern (normal distribution). These are traditional methods. The advantage is the clarity of the calculation process. However, they have strict requirements on data distribution and are only applied to small data volumes and simple solutions.\n\n\n6.3.2.2 Non-parametric methods\nThey can apply to complex calculation processes, do not rely on the data distribution and are very popular now. They are computation-heavy and require a large amount of data to train the model.\nML methods have high accuracy but are poor in interpretation. It is difficult to describe the reasons for classification. Overfitting is also a main problem in ML. So, in the process of using an ML model, it is important to choose a suitable model using reasonable hyperparameters. When building training and test sets, it is necessary to ensure the classification’s generalisation ability. For example, we tried different ways to choose training and test sets. And get different accuracy.\n\n// Sorting by polygons\nvar withRandom = polygons.randomColumn('random');\nvar split = 0.7;\nvar trainingPartition = withRandom.filter(ee.Filter.lt('random', split));\nvar testingPartition = withRandom.filter(ee.Filter.gte('random', split));\n\nprint(trainingPartition, \"train\")\nprint(testingPartition, \"test\")\n\n\n// Sorting by pixel\nvar pixel_number= 1000;\n\nvar urban_low_points=ee.FeatureCollection.randomPoints(urban_low, pixel_number).map(function(i){\n  return i.set({'class': 1})})\n  \nvar water_points=ee.FeatureCollection.randomPoints(water, pixel_number).map(function(i){\n  return i.set({'class': 2})})\n  \nvar urban_high_points=ee.FeatureCollection.randomPoints(urban_high, pixel_number).map(function(i){\n  return i.set({'class': 3})})\n\nvar grass_points=ee.FeatureCollection.randomPoints(grass, pixel_number).map(function(i){\n  return i.set({'class': 4})})\n\nvar bare_earth_points=ee.FeatureCollection.randomPoints(bare_earth, pixel_number).map(function(i){\n  return i.set({'class': 5})})\n\nvar forest_points=ee.FeatureCollection.randomPoints(forest, pixel_number).map(function(i){\n  return i.set({'class': 6})})\n\nvar point_sample=ee.FeatureCollection([urban_low_points,\n                                  water_points,\n                                  urban_high_points,\n                                  grass_points,\n                                  bare_earth_points,\n                                  forest_points])\n                                  .flatten()\n                                  .randomColumn();\n                                    \nvar split=0.7\nvar training_sample = point_sample.filter(ee.Filter.lt('random', split));\nvar validation_sample = point_sample.filter(ee.Filter.gte('random', split));",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification Ⅰ</span>"
    ]
  },
  {
    "objectID": "Wk7_Clas1.html#application",
    "href": "Wk7_Clas1.html#application",
    "title": "6  Classification Ⅰ",
    "section": "6.2 Application",
    "text": "6.2 Application\nRemotely sensing image classification is an important method of feature identity. Asbestos-containing material (ACM) roofs were widely used in many residential buildings because of their good heat preservation properties. However, airborne asbestos can cause serious damage to human health, especially to the respiratory system. Recently, Italy and Poland have introduced relative policies: Progressive removal of ACM roofs. Remote sensing imagery can help with the implementation of policies quickly and effectively. Using high-resolution images (like Sentinel-2) can help the government remotely monitor the distribution of ABM roofs. This reduces the cost of surveys in the field.\n\n\n\nThe general progress of the studies of ACM roof mapping Source:Abbasi et al. (2022)\n\n\nBecause the material of roofs has similar spectral properties. Therefore, identification of ACM needs higher resolution and spectral requirements. Hyperspectral imaging (HIS) has been widely used in studies of ACM roof mapping. There are lots of pixel-based image analysis (PBIA) classifiers being used in studies like spectral angle mapper, convolutional neural networks (CNNs) and random forest (RF). Accuracy varies from method to method. Convolutional neural networks (CNNs) seem to have the highest accuracy.\n\n\n\nThe summary of PBIA for ACM studies Source:Abbasi et al. (2022)\n\n\nTommasini, Bacciottini, and Gherardelli (2019a) analysed WorldView-3 satellite imagery using Pansharpening and the Roof Classify Plugin in QGIS. This is a supervised classification method that requires the preparation of classified files and images with labels. The author applied the Random Forest (RF) for classification, achieving a 90% detection rate for ACM roofs. But PBIA has its limitations: due to spectral variations, pixels on the same roof may be misclassified. This leads to a loss of accuracy.\n\n\n\nA classified image using RF Source:Tommasini, Bacciottini, and Gherardelli (2019b)\n\n\nDeep learning is also applied to roof identification. It relies on its ability to handle large amounts of data well. DL can be analysed more accurately in the studies of ACM roof mapping, which demands high-resolution data. CNNs can automatically learn complex spatial features through hierarchical convolutional operations. Krówczyńska et al. (2020) used CNNs to train the data. The OA of the training and the validation of the training were 93% and 86%. It performs better than RF and SAM. CNNs can ignore the influences from the neighbourhood pixel can reduce PBIA errors. So, it is more suitable for studies about urban roof cover identification. In the future, we can use multi-temporal remote sensing data to monitor ACM roof changing trends. It helps us learn about the effectiveness of the implementation of government policies.\n\n\n\nThe result of the classification of ACM foofs using CNNs Source:Krówczyńska et al. (2020)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification Ⅰ</span>"
    ]
  },
  {
    "objectID": "Wk7_Clas1.html#reflection",
    "href": "Wk7_Clas1.html#reflection",
    "title": "6  Classification Ⅰ",
    "section": "6.3 Reflection",
    "text": "6.3 Reflection\nThis week, I learned the application of ML in remote sensing image classification. In particular, deep learning methods have a high accuracy for feature recognition. Through remote sensing image classification, the land cover can be identified easily. It can be used in roof cover identification to reflect the results of relative urban renewal policies like the removal of ACM roofs. It can help monitor policy implementation. Remote sensing images are also valuable for disaster assessment, such as evaluating infrastructure damage caused by wars. However, it is often limited by data accessibility and computational cost. (Mueller et al. 2021)\nHowever, to identify the building-level images, we need high-resolution and a huge amount of computation, which may cost a lot of money. That is opposite to the low-cost and large-scale monitoring, which are the original goals of using remote sensing data sets. Therefore, Future research should focus on improving the classification accuracy of open-source and low-resolution datasets. Using methods like super-resolution techniques or image enhancement (like texture) can enhance the identification of low-resolution satellite imagery, the SAR data could provide complementary information. Through these methods, the remote sensing data can be used in more areas to monitor the policy results. The lower costs and easy methods to get data will enable more developing countries to use remote sensing for policy monitoring and disaster recovery. This can help with expanding the global applicability of remote sensing image classifications.\n\n\n\n\nAbbasi, Mohammad, Sherif Mostafa, Abel Silva Vieira, Nicholas Patorniti, and Rodney A. Stewart. 2022. “Mapping Roofing with Asbestos-Containing Material by Using Remote Sensing Imagery and Machine Learning-Based Image Classification: A State-of-the-Art Review.” Sustainability 14 (13). https://doi.org/10.3390/su14138068.\n\n\nKrówczyńska, Małgorzata, Edwin Raczko, Natalia Staniszewska, and Ewa Wilk. 2020. “Asbestoscement Roofing Identification Using Remote Sensing and Convolutional Neural Networks (CNNs).” Remote Sensing 12 (3): 408. https://doi.org/10.3390/rs12030408.\n\n\nMueller, Hannes, Andre Groeger, Jonathan Hersh, Andrea Matranga, and Joan Serrat. 2021. “Monitoring War Destruction from Space Using Machine Learning.” Proceedings of the National Academy of Sciences of the United States of America 118 (23): 1–9. https://www.jstor.org/stable/27040853.\n\n\nTommasini, Maurizio, Alessandro Bacciottini, and Monica Gherardelli. 2019b. “A QGIS Tool for Automatically Identifying Asbestos Roofing.” ISPRS International Journal of Geo-Information 8 (3). https://doi.org/10.3390/ijgi8030131.\n\n\n———. 2019a. “A QGIS Tool for Automatically Identifying Asbestos Roofing.” ISPRS International Journal of Geo-Information 8 (3). https://doi.org/10.3390/ijgi8030131.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification Ⅰ</span>"
    ]
  },
  {
    "objectID": "Wk8_Clas2.html",
    "href": "Wk8_Clas2.html",
    "title": "7  Classification Ⅱ",
    "section": "",
    "text": "7.1 Summary\nThis week, we learned about land cover classifications and accuracy. The first part is about advanced classification methods, the second part is about Error Matrix, and the third part is about cross-validation.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification Ⅱ</span>"
    ]
  },
  {
    "objectID": "Wk8_Clas2.html#summary",
    "href": "Wk8_Clas2.html#summary",
    "title": "7  Classification Ⅱ",
    "section": "",
    "text": "7.1.1 Advanced classification methods\nClassification methods include Pixel-Based Image Analysis and Object-Based Image Analysis. PBIA relies on the spectral properties of pixels for classification. “OBIA groups pixels into representative vector shapes with size and geometry” (“Image Classification Techniques in Remote Sensing,” n.d.), making classification results more spatially continuous. Sub-pixel analysis is a branch of PBIA, and it is used to solve the problem of pixel size that is bigger than the target object. I interpret the OBIA as a jigsaw puzzle that is in a high-resolution image, objects are composed of multiple pixels. Sub-pixel analysis is like a kind of colour palette that is in a low-resolution image, pixels are composed of multiple objects. Usually, OBIA is suitable for the high-quality classification of high-resolution images, and sub-pixel analysis applies to the mixed-pixel problem of low-resolution data. The following table will list how they are used.\n\n\n\nOBIA vs Sub-pixel analysis\n\n\nCriteria\nObject-Based Image Analysis (OBIA)\nSub-pixel Analysis\n\n\n\n\nBasic Unit\nObject (multiple pixels)\nMixed pixel (one pixel contains multiple land cover types)\n\n\nResolution\nHigh-resolution (UAV, WorldView)\nLow to medium resolution (MODIS, Landsat)\n\n\nClassification\nMachine learning (SVM, Decision Tree, KNN, Random Forest)\nSpectral unmixing (LSU, MESMA, V-I-S model)\n\n\nFeatures Used\nSpectral, spatial, shape-related (length-width ratio, edge length, compactness)\nSpectral only\n\n\nMain Methods\nSegmentation: SLIC, Watershed, Multi-resolution Segmentation\nUnmixing: Linear Spectral Unmixing (LSU), MESMA\n\n\nModel\nClassification: SVM, Random Forest, Decision Tree, KNN\nLand Cover Fractions: V-I-S model\n\n\nApplications\nUrban classification, land cover mapping\nMixed land cover analysis, vegetation fraction estimation\n\n\nComplexity\nHigh (segmentation + classification)\nLow (spectral analysis only)\n\n\nAdvantages\nHigh accuracy, noise reduction\nEstimates sub-pixel land cover proportions\n\n\nLimitations\nRequires segmentation, preprocessing\nSensitive to endmember selection\n\n\n\n\n\n\n\n\n\n\nPBIA vs OBIA Source: GISgeography\n\n\n\n\n7.1.2 Accuracy\nBased on the error matrix, there are many indexes and ways to measure the accuracy of the classification results. I think this part can be understood in multi-levels. Basically, the accuracy is calculated by the error matrix with the overall accuracy, user accuracy, and producer accuracy. These methods can measure most situations but may be incorrect on data with unevenly distributed categories. The Kappa coefficient can solve this problem to some extent. But when there is an extreme imbalance in the distribution of a category, it will still overestimate model performance. (Comber et al. 2012) F1 and ROC curves can also help with measuring model accuracy.\n\n\n\nTerms related to accuracy assessment\n\n\nTerm\nDefinition\nFormula / Key Info\n\n\n\n\nOverall Accuracy (OA)\nPercentage of correctly classified samples among all samples.\nTP + TN / (TP + FP + FN + TN)\n\n\nProducer’s Accuracy (PA)\nProbability that a real land cover class is correctly classified.\nTP / (TP + FN)\n\n\nUser’s Accuracy (UA)\nProbability that a predicted class is actually correct.\nTP / (TP + FP)\n\n\nOmission Error\nHow often a real class is missed.\nFN / (TP + FN)\n\n\nCommission Error\nHow often a predicted class is incorrect.\nFP / (TP + FP)\n\n\nKappa Coefficient (Kappa, κ)\nMeasures agreement between predicted and actual classes, adjusting for chance agreement.\nκ = (p_o - p_e) / (1 - p_e), where p_o is observed accuracy, p_e is expected accuracy.\n\n\nF1-Score\nHarmonic mean of precision and recall, balancing both metrics.\n2 × (Precision × Recall) / (Precision + Recall)\n\n\nConfusion Matrix\nA table showing actual vs. predicted classifications, used to compute accuracy metrics.\nContains TP, FP, FN, TN values.\n\n\nTrue Positive (TP)\nCorrectly classified positive cases.\nExample: Correctly classified urban areas.\n\n\nFalse Positive (FP)\nIncorrectly classified positive cases.\nExample: A forest misclassified as urban.\n\n\nTrue Negative (TN)\nCorrectly classified negative cases.\nExample: Correctly classified non-urban areas.\n\n\nFalse Negative (FN)\nIncorrectly classified negative cases.\nExample: An urban area misclassified as non-urban.\n\n\nCross Validation (CV)\nSplitting data into training and validation sets to evaluate model performance.\nCommon methods: K-Fold, Leave-One-Out (LOO), Spatial CV.\n\n\nK-Fold Cross Validation\nDivides data into K groups, using each fold as a test set once.\nCommon choices: 5-Fold, 10-Fold.\n\n\nSpatial Cross Validation\nEnsures training and test data are spatially separated to avoid overfitting.\nUses spatial partitioning (e.g., K-Means, Voronoi).\n\n\n\n\n\n\n\n\n\n7.1.3 Cross-validation\nImage classification can be viewed as a complete process of machine learning, so it can use cross-validation to test the accuracy of the model. However, remote sensing is special because neighbouring pixels may have a spatial correlation. So, using regular methods will give the false belief that the model is too accurate. Spatial cross-validation can accurately assess the accuracy of image classification.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification Ⅱ</span>"
    ]
  },
  {
    "objectID": "Wk8_Clas2.html#application",
    "href": "Wk8_Clas2.html#application",
    "title": "7  Classification Ⅱ",
    "section": "7.2 Application",
    "text": "7.2 Application\nThis week we compared the differences between PBIA and OBIA through two articles on agricultural landscape classification. Duro, Franklin, and Dubé (2012) utilised a moderate-resolution image (10m) to identify Canada’s agricultural landscape, which includes cropland, grassland, soil, and wetland. A comparison of the results of PBIA and OBIA was conducted using three classification methods (DT, SVM, and RF), showing no significant differences (p &gt; 0.05). Wetlands are an exception: OBIA performs more accurately than PBIA in wetland identification, possibly due to the unique characteristics of wetlands. The gradual blurring and complex boundaries of wetlands make their identification challenging through pixel-based analysis. In contrast, OBIA can integrate neighboring pixels, providing a more generalized visual appearance and a more continuous description of land cover.As a result OBIA is more accurate in identifying wetlands.\n\n\n\n\n\nPBIA RF Source: Duro, Franklin, and Dubé (2012)\n\n\n\n\n\n\n\nOBIA RF Source: Duro, Franklin, and Dubé (2012)\n\n\n\n\nCastillejo-González et al. (2009) concluded that OBIA achieved higher accuracy than PBIA in identifying agricultural landscapes. The reasons for the different conclusions may be because: First, the resolution of the image is higher (3m). Second, the agricultural landscapes are more carefully categorised. It not only includes bare soil and cropland but also divides cropland into olive orchards, vineyards and sunflower fields. So, it has higher requirements for classification accuracy. Third, the classification methods employed by the author are traditional approaches, including MD, MC and SAM, unlike Duro, who used machine learning methods. Finally, for measuring accuracy, Castillejo-González used the kappa coefficients, which may lead to some errors in the results due to the uneven distribution of the categories. However, Duro used spatial cross-validation k-fold, which is more suitable for remote sensing data.\n\n\n\nPBIA vs OBIA :Castillejo-González et al. (2009)\n\n\nIn summary, for most open-source data today, both PBIA and OBIA, when applied with machine learning, will produce similar results. However. CNNs are able to automatically learn more complex spatial features through convolutional operations and the information from surrounding pixels. That advanced algorithm bridges the gap between PBIA and OBIA. OBIA requires more computing power and time because of image segmentation. I think that in general situations, PBIA using ML methods is enough for most image classification missions with high efficiency. OBIA can be used for heterogeneous landscapes with unclear boundaries, like wetlands. Now, there are many studies that combine PBIA and OBIA together.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification Ⅱ</span>"
    ]
  },
  {
    "objectID": "Wk8_Clas2.html#reflection",
    "href": "Wk8_Clas2.html#reflection",
    "title": "7  Classification Ⅱ",
    "section": "7.3 Reflection",
    "text": "7.3 Reflection\nIn comparing PBIA and OBIA, I have observed that different methods yield different results depending on the data resolution. Additionally, I have recognized the importance of balancing computational cost with accuracy. Machine learning methods, in particular, are widely used for image classification. Achieving a balance between effectiveness and accuracy is also crucial for policy implementation. For example, is OBIA combined with simple machine learning methods, such as Random Forest (RF), better than PBIA combined with deep learning (CNN) for the same object classification task? This question confuses me. What is the appropriate trade-off between the cost and accuracy of classification of these two methods? In the future, There may be more ways to effectively combine PBIA and OBIA. These methods will widely be used in environmental monitoring, urban planning, and agricultural mapping. Moreover, because the authors use different methods to measure the accuracy of classification, their results need to be further verified. Did the author use cross-validation or Kappa? What is the specific error matrix for the classification model? Does the author consider spatial autocorrelation? The distribution of classifications also requires attention; do the prediction results have an uneven distribution? We should be careful when we read articles. The result in a high OA does not mean the categorisation is good.\nI find this content really interesting because it emphasizes the limitations of classification techniques in applications, perhaps through hybrid models that combine their strengths. In the future, improving methods for assessing the accuracy of remote sensing classification could enhance the credibility of research findings. This reflection has made me more aware of the complexity of classification methodologies and the need for careful evaluation when applying them to geospatial problems, particularly in fields where policy decisions rely on accurate spatial analysis.\n\n\n\n\nCastillejo-González, Isabel Luisa, Francisca López-Granados, Alfonso García-Ferrer, José Manuel Peña-Barragán, Montserrat Jurado-Expósito, Manuel Sánchez De La Orden, and María González-Audicana. 2009. “Object- and Pixel-Based Analysis for Mapping Crops and Their Agro-Environmental Associated Measures Using QuickBird Imagery.” Computers and Electronics in Agriculture 68 (2): 207–15. https://doi.org/10.1016/j.compag.2009.06.004.\n\n\nComber, Alexis, Peter Fisher, Chris Brunsdon, and Abdulhakim Khmag. 2012. “Spatial analysis of remote sensing image classification accuracy遥感影像分类精度的空间分析.” Remote Sensing of Environment 127 (December): 237–46. https://doi.org/10.1016/j.rse.2012.09.005.\n\n\nDuro, Dennis C., Steven E. Franklin, and Monique G. Dubé. 2012. “A Comparison of Pixel-Based and Object-Based Image Analysis with Selected Machine Learning Algorithms for the Classification of Agricultural Landscapes Using SPOT-5 HRG Imagery.” Remote Sensing of Environment 118 (March): 259–72. https://doi.org/10.1016/j.rse.2011.11.020.\n\n\n“Image Classification Techniques in Remote Sensing.” n.d. https://gisgeography.com/image-classification-techniques-remote-sensing/.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification Ⅱ</span>"
    ]
  },
  {
    "objectID": "Wk9_SAR.html",
    "href": "Wk9_SAR.html",
    "title": "8  Syntactic Aperture Radar",
    "section": "",
    "text": "8.1 Summary",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Syntactic Aperture Radar</span>"
    ]
  },
  {
    "objectID": "Wk9_SAR.html#summary",
    "href": "Wk9_SAR.html#summary",
    "title": "8  Syntactic Aperture Radar",
    "section": "",
    "text": "8.1.1 SAR base background and advantages\nSynthetic Aperture Radar (SAR) is an active remote sensing sensor. Unlike passive sensors that rely on solar radiation, SAR actively emits its own electromagnetic waves and forms images by receiving the backscattered signals from the Earth’s surface. This mechanism makes SAR unaffected by cloud and weather, allowing for all-weather observation. SAR improves spatial resolution by synthesising backscatter from multiple observation angles as the sensor moves along its path. In Week 4, I analysed OE data related to flood monitoring. Compared to Landsat 8, SAR data provides greater continuity and reliability, especially in disaster scenarios. This is because SAR can penetrate cloud cover and reduce data loss caused by cloudy or rainy conditions.\n\n\n8.1.2 SAR data types and parameter selection\nSAR data characteristics are primarily defined by wavelength (band) and polarisation. Common bands include X, C, and L. Wavelength determines the radar’s penetration ability: short-wavelength bands (e.g., X-band) are suitable for monitoring surface features, while long-wavelength bands (e.g., L-band) can penetrate vegetation, allowing for the analysis of forest structure.\n\n\n\nSAR bands source:NASA Earth Data\n\n\nPolarisation refers to the orientation of the radar wave’s oscillation. Different surface types respond differently to each polarisation mode: VV is sensitive to rough surface scattering, while VH is better suited for detecting volume scattering (e.g., vegetation). The selection of band-polarisation combinations should be made with care for the specific analytical objective.\n\n\n\nPolarisation source:NASA Earth Data\n\n\nSAR backscatter intensity can be expressed in three main formats: power, amplitude, and dB, each with different advantages and applications due to SAR’s wide dynamic range.\n\n\n\nSAR backscatter intensity\n\n\nExpression Format\nFormula (Visual)\nAdvantages\nDisadvantages\nSuitable Use\n\n\n\n\nPower\nI² + Q²\nMost original and accurate representation of radar backscatter; linear values; suitable for statistical and ML analysis\nVery small values; low contrast between bright and dark areas; poor for visualization\nData analysis, modeling, classification\n\n\nAmplitude\n√(I² + Q²)\nNatural-looking brightness range in images; easy for visual interpretation\nStill linear; large dynamic range; less detail enhancement compared to dB\nImage processing, visual analysis\n\n\ndB\n10 × log₁₀(Power)\nCompressed dynamic range; easier to observe details in dark regions (e.g., water); default format in GEE\nNot linear; unsuitable for statistical/scientific calculation; bright areas may appear saturated\nImage visualization; highlighting differences/contrast\n\n\n\n\n\n\n\n\n\n8.1.3 SAR data application\nSAR applications often rely on comparing images acquired at different times. Differential InSAR (DInSAR) detects ground movement by measuring phase differences between two SAR images acquired at different times. It is commonly used to monitor earthquakes and land subsidence. There are several techniques for detecting changes using SAR data. Unlike optical data, SAR images cannot be simply subtracted due to their multiplicative noise and different statistical characteristics. Commonly used methods include ratio, improved ratio, and log-ratio techniques.\nTo enhance classification accuracy, SAR imagery is often fused with optical data. Fusion strategies—ranging from decision-level to pixel-level—combine the complementary strengths of both datasets. While decision-level fusion is more accessible and commonly used in GEE, I realised that more complex fusion techniques offer higher precision and will likely be essential in future applications such as agricultural monitoring.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Syntactic Aperture Radar</span>"
    ]
  },
  {
    "objectID": "Wk9_SAR.html#application",
    "href": "Wk9_SAR.html#application",
    "title": "8  Syntactic Aperture Radar",
    "section": "8.2 Application",
    "text": "8.2 Application\nRemote sensing data play a vital role not only in scientific analysis but also in humanitarian disaster assessment. This week’s lecture illustrates how SAR data can be applied to assess war-related damage. Earth observation (EO) satellite missions offer a cost-effective and repeatable solution for monitoring war impacts. Due to their relatively low spatial resolution, the Landsat series enables faster image processing and lower computational demands. However, its spatial resolution is insufficient for detecting changes at the building level. As a result, it is difficult to assess the extent of structural damage in urban areas. Therefore, war monitoring using Landsat data has focused on broader environmental changes, lacking direct insight into urban structural damage (e.g., buildings). That limits the ability to quantitatively assess socio-economic losses. (Kaplan et al. 2022)\n\n\n\npre- and post-war changes :Kaplan et al. (2022)\n\n\nSentinel-1 and Sentinel-2 have higher resolution, so they are widely used in the study of urban change and damage to buildings caused by war. Most of the studies apply machine learning techniques to extract image features from Sentinel-2 and polarization features (e.g., VH, HH) from SAR data. Then, classifiers are used to evaluate building damage and classify war-affected areas.(Aimaiti et al. 2022; Li, Guo, and Chan 2025; Fakhri and Gkanatsios 2021) As previously discussed in Week 3, image features play a key role in urban classification and object identification. In terms of classification accuracy, MS-based data generally has higher overall accuracy than SAR-based data. The VH polarization in SAR data plays the most important role in detecting destroyed buildings. This may be due to their higher sensitivity to structural variations. SAR data is affected by speckle noise at low resolution. MS data captures features well but responds poorly to abrupt structural changes. Li, Guo, and Chan (2025) applied a Random Forest model to classify war-related features using fused Sentinel-1 SAR and Sentinel-2 MS data. Fused data achieved 95.91% accuracy, outperforming MS (95.01%) and SAR (78.65%).\n\n\n\nFeatures Importance For Destroyed Building :Li, Guo, and Chan (2025)\n\n\nAlthough many articles emphasise the advantages of SAR’s independence from weather. I think the spatial distribution and extent of damage to the building are more important than continuous monitoring over time. Aimaiti et al. (2022) performed change detection by calculating the log ratio of backscatter intensity and GLCM texture difference. The process then continued with classification, avoiding two-stage model training and improving overall efficiency.\n\n\n\nSentinel-1 vs Sentinel-2 :Aimaiti et al. (2022)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Syntactic Aperture Radar</span>"
    ]
  },
  {
    "objectID": "Wk9_SAR.html#reflection",
    "href": "Wk9_SAR.html#reflection",
    "title": "8  Syntactic Aperture Radar",
    "section": "8.3 Reflection",
    "text": "8.3 Reflection\nSAR is now widely used in disaster assessment, including natural and war-related disasters. SAR is characterised by insensitivity to weather conditions, short acquisition cycles and high structural responsiveness. This week’s readings highlighted the importance of intensity ratios and polarization features in SAR-based analysis. The phase is also useful; it can detect slight ground movement. Interferometric SAR (InSAR) techniques have already been used to detect landslides, due to their ability to capture small-scale deformations. (Xue et al. 2018)Is it possible to use InSAR to identify localised ground disturbances caused by war (e.g. blast impacts)? Does that mean InSAR can be applied to war damage assessment? Through this week’s lecture, I gained knowledge about the importance of remote sensing images in disaster assessment. Automated remote sensing processes can provide disaster damage assessments in the first instance. It provides scientific evidence for the timely allocation of relief resources and thus better reflects the humanitarian spirit. Moreover, the data discussed this week is open-source, and its application helps reduce the development gap between countries. In the future, an open framework for international cooperation on disaster assessment could be developed, which potentially contributes to the scientific coordination of humanitarian aid. Although such ideas face practical challenges such as data privacy and national sovereignty, their potential merits warrant further exploration.\n\n\n\n\nAimaiti, Yusupujiang, Christina Sanon, Magaly Koch, Laurie G. Baise, and Babak Moaveni. 2022. “War Related Building Damage Assessment in Kyiv, Ukraine, Using Sentinel-1 Radar and Sentinel-2 Optical Images.” Remote Sensing 14 (24): 6239. https://doi.org/10.3390/rs14246239.\n\n\nFakhri, Falah, and Ioannis Gkanatsios. 2021. “Integration of Sentinel-1 and Sentinel-2 Data for Change Detection: A Case Study in a War Conflict Area of Mosul City.” Remote Sensing Applications: Society and Environment 22 (April): 100505. https://doi.org/10.1016/j.rsase.2021.100505.\n\n\nKaplan, Gordana, Tatjana Rashid, Mateo Gasparovic, Andrea Pietrelli, and Vincenzo Ferrara. 2022. “Monitoring War-Generated Environmental Security Using Remote Sensing: A Review.” Land Degradation and Development 33 (10): 1513–26. https://doi.org/10.1002/ldr.4249.\n\n\nLi, Xinchen, Liang Guo, and Jonathan Cheung-Wai Chan. 2025. “Combined Sentinel-1 and Sentinel-2 Imagery for Destroyed Building Classification in Gaza Strip with Random Forest.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 18: 3827–39. https://doi.org/10.1109/JSTARS.2024.3522389.\n\n\nXue, D., X. Yu, S. Jia, F. Chen, and X. Li. 2018. “Study on Landslide Disaster Extraction Method Based on Spaceborne Sar Remote Sensing Images  Take Alos Palsar for an Example.” The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences XLII-3 (April): 2023–27. https://doi.org/10.5194/isprs-archives-XLII-3-2023-2018.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Syntactic Aperture Radar</span>"
    ]
  },
  {
    "objectID": "Wk7_Clas1.html#summary",
    "href": "Wk7_Clas1.html#summary",
    "title": "6  Classification Ⅰ",
    "section": "",
    "text": "6.1.1 Conclusion of the process of analysing remotely sensed images\nCombining the previous studies and articles, I think the analysis of remotely sensed images includes: 1. Data obtained – 2 pre-processed- 3. image enhancement-4. Factor choosing -5. Predict modelling(classification/regression)-6. Result assessment. Image classification belongs to the fifth step, which directly affects the final research results.\n\n\n6.1.2 Approaches to image classification\n\n6.1.2.1 Unsupervised classification\nThese methods are applied to identify data with unknown land cover classes. Data were labelled into different categories using ML methods like clustering (k-means, DB-scan) based on spectral features. This reduces the influence of the human factor. The initial result of unsupervised classification may have mixed categories, so it always needs downscaling methods like PCA to optimise.\n\n\n\nUnsupervised classification. Source: GISgeography\n\n\n\n\n6.1.2.2 Supervised classification\nThis methods are the Classifier learns pattern in the data and puts the labels onto new data. It includes Parametric methods and Non-parametric methods. Recent studies tend to use machine learning/expert systems or spectral hybrid analyses. The steps in supervised classification include class definition, pre-processing, training, pixel assignment, and accuracy assessment.\n\n\n\nSupervised classification. Source: GISgeography\n\n\n\n6.1.2.2.1 Parametric methods\nThey are based on the assumption that the data follow a certain distribution pattern (normal distribution). These are traditional methods. The advantage is the clarity of the calculation process. However, they have strict requirements on data distribution and are only applied to small data volumes and simple solutions.\n\n\n6.1.2.2.2 Non-parametric methods\nThey can apply to complex calculation processes, do not rely on the data distribution and are very popular now. They are computation-heavy and require a large amount of data to train the model.\nML methods have high accuracy but are poor in interpretation. It is difficult to describe the reasons for classification. Overfitting is also a main problem in ML. So, in the process of using an ML model, it is important to choose a suitable model using reasonable hyperparameters. When building training and test sets, it is necessary to ensure the classification’s generalisation ability. In practice, we tried different ways to choose training and test sets. And get different accuracy.\n\n// Sorting by polygons\nvar withRandom = polygons.randomColumn('random');\nvar split = 0.7;\nvar trainingPartition = withRandom.filter(ee.Filter.lt('random', split));\nvar testingPartition = withRandom.filter(ee.Filter.gte('random', split));\n\nprint(trainingPartition, \"train\")\nprint(testingPartition, \"test\")\n\n\n// Sorting by pixel\nvar pixel_number= 1000;\n\nvar urban_low_points=ee.FeatureCollection.randomPoints(urban_low, pixel_number).map(function(i){\n  return i.set({'class': 1})})\n  \nvar water_points=ee.FeatureCollection.randomPoints(water, pixel_number).map(function(i){\n  return i.set({'class': 2})})\n  \nvar urban_high_points=ee.FeatureCollection.randomPoints(urban_high, pixel_number).map(function(i){\n  return i.set({'class': 3})})\n\nvar grass_points=ee.FeatureCollection.randomPoints(grass, pixel_number).map(function(i){\n  return i.set({'class': 4})})\n\nvar bare_earth_points=ee.FeatureCollection.randomPoints(bare_earth, pixel_number).map(function(i){\n  return i.set({'class': 5})})\n\nvar forest_points=ee.FeatureCollection.randomPoints(forest, pixel_number).map(function(i){\n  return i.set({'class': 6})})\n\nvar point_sample=ee.FeatureCollection([urban_low_points,\n                                  water_points,\n                                  urban_high_points,\n                                  grass_points,\n                                  bare_earth_points,\n                                  forest_points])\n                                  .flatten()\n                                  .randomColumn();\n                                    \nvar split=0.7\nvar training_sample = point_sample.filter(ee.Filter.lt('random', split));\nvar validation_sample = point_sample.filter(ee.Filter.gte('random', split));",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification Ⅰ</span>"
    ]
  }
]